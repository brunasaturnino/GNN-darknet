{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria√ß√£o arquivos lookup ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados de tr√°fego...\n",
      "Arquivos ip_lookup.json e port_lookup.json recriados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Carregamento dos Dados de Tr√°fego\n",
    "print(\"Carregando dados de tr√°fego...\")\n",
    "df_trafego = pd.read_csv(\"trafego_rede_simulado.csv\")\n",
    "\n",
    "# Criar diret√≥rio se n√£o existir\n",
    "os.makedirs(\"../data/graph\", exist_ok=True)\n",
    "\n",
    "# Gerar lookup para IPs √∫nicos\n",
    "unique_ips = pd.concat([df_trafego['src_ip'], df_trafego['dst_ip']]).unique()\n",
    "ip_lookup = {str(ip): idx for idx, ip in enumerate(unique_ips)}\n",
    "\n",
    "# Gerar lookup para portas √∫nicas\n",
    "unique_ports = df_trafego['dst_port'].unique()\n",
    "port_lookup = {str(port): idx for idx, port in enumerate(unique_ports)}\n",
    "\n",
    "# Salvar os dicion√°rios como JSON\n",
    "with open(\"../data/graph/ip_lookup.json\", \"w\") as file:\n",
    "    json.dump(ip_lookup, file, indent=4)\n",
    "\n",
    "with open(\"../data/graph/port_lookup.json\", \"w\") as file:\n",
    "    json.dump(port_lookup, file, indent=4)\n",
    "\n",
    "print(\"Arquivos ip_lookup.json e port_lookup.json recriados com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria√ß√£o do corpus e das features ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados de tr√°fego...\n",
      "         src_ip        dst_ip  pacotes  bytes_transferidos protocolo  \\\n",
      "0   192.168.1.3  192.168.1.27      981               51284       UDP   \n",
      "1  192.168.1.33  192.168.1.34      476               62016      ICMP   \n",
      "2   192.168.1.6  192.168.1.27      702               74072       TCP   \n",
      "3  192.168.1.50  192.168.1.17      634               62287       UDP   \n",
      "4  192.168.1.10  192.168.1.43      558               13084       TCP   \n",
      "\n",
      "   interval   ts  dst_port  \n",
      "0        14  1.0     15579  \n",
      "1        18  2.0     12631  \n",
      "2        18  3.0     39440  \n",
      "3        29  4.0      6357  \n",
      "4         7  5.0     37372  \n",
      "Carregando dados de Ground Truth...\n",
      "Calculando estat√≠sticas gerais...\n",
      "                 src_ip  destino  pkts\n",
      "label                                 \n",
      "benign                1        6  2740\n",
      "censys                1        6  2968\n",
      "internetcensus        1        6  3473\n",
      "mirai                 2       11  6019\n",
      "securitytrails        1        4  4066\n",
      "unk_bruteforcer       1        6  4094\n",
      "Carregando matriz de adjac√™ncia...\n",
      "Total de N√≥s (IPs distintos): 50\n",
      "Total de Arestas (Conex√µes na Rede): 191\n",
      "Carregando dicion√°rios de lookup...\n",
      "Extraindo features dos n√≥s...\n",
      "Features extra√≠das com sucesso:\n",
      "   origem_id  media_pkts  soma_pkts\n",
      "0          4  554.166667       3325\n",
      "1          5  384.857143       2694\n",
      "2          8  578.833333       3473\n",
      "3         14  456.666667       2740\n",
      "4         16  677.666667       4066\n",
      "Features do dia 18 salvas em '../data/features/features_18.csv'.\n",
      "Features do dia 7 salvas em '../data/features/features_7.csv'.\n",
      "Features do dia 17 salvas em '../data/features/features_17.csv'.\n",
      "Features do dia 22 salvas em '../data/features/features_22.csv'.\n",
      "Features do dia 2 salvas em '../data/features/features_2.csv'.\n",
      "Features do dia 16 salvas em '../data/features/features_16.csv'.\n",
      "Features do dia 6 salvas em '../data/features/features_6.csv'.\n",
      "Features do dia 26 salvas em '../data/features/features_26.csv'.\n",
      "Features do dia 28 salvas em '../data/features/features_28.csv'.\n",
      "Features do dia 24 salvas em '../data/features/features_24.csv'.\n",
      "Features do dia 29 salvas em '../data/features/features_29.csv'.\n",
      "Features do dia 31 salvas em '../data/features/features_31.csv'.\n",
      "Features do dia 13 salvas em '../data/features/features_13.csv'.\n",
      "Features do dia 9 salvas em '../data/features/features_9.csv'.\n",
      "Features do dia 3 salvas em '../data/features/features_3.csv'.\n",
      "Features do dia 21 salvas em '../data/features/features_21.csv'.\n",
      "Features do dia 19 salvas em '../data/features/features_19.csv'.\n",
      "Features do dia 10 salvas em '../data/features/features_10.csv'.\n",
      "Features do dia 30 salvas em '../data/features/features_30.csv'.\n",
      "Features do dia 14 salvas em '../data/features/features_14.csv'.\n",
      "Features do dia 20 salvas em '../data/features/features_20.csv'.\n",
      "Features do dia 1 salvas em '../data/features/features_1.csv'.\n",
      "Features do dia 4 salvas em '../data/features/features_4.csv'.\n",
      "Features do dia 11 salvas em '../data/features/features_11.csv'.\n",
      "Features do dia 5 salvas em '../data/features/features_5.csv'.\n",
      "Features do dia 15 salvas em '../data/features/features_15.csv'.\n",
      "Features do dia 27 salvas em '../data/features/features_27.csv'.\n",
      "Features do dia 8 salvas em '../data/features/features_8.csv'.\n",
      "Features do dia 23 salvas em '../data/features/features_23.csv'.\n",
      "Features do dia 25 salvas em '../data/features/features_25.csv'.\n",
      "Features do dia 12 salvas em '../data/features/features_12.csv'.\n",
      "Extra√ß√£o e salvamento de features conclu√≠dos! üöÄ\n",
      "Analisando tr√°fego para NLP...\n",
      "Gerando corpus NLP...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e474e8370f4ab88752e2ad69180c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus do dia 1 salvo com sucesso.\n",
      "Corpus do dia 2 salvo com sucesso.\n",
      "Corpus do dia 3 salvo com sucesso.\n",
      "Corpus do dia 4 salvo com sucesso.\n",
      "Corpus do dia 5 salvo com sucesso.\n",
      "Corpus do dia 6 salvo com sucesso.\n",
      "Corpus do dia 7 salvo com sucesso.\n",
      "Corpus do dia 8 salvo com sucesso.\n",
      "Corpus do dia 9 salvo com sucesso.\n",
      "Corpus do dia 10 salvo com sucesso.\n",
      "Corpus do dia 11 salvo com sucesso.\n",
      "Corpus do dia 12 salvo com sucesso.\n",
      "Corpus do dia 13 salvo com sucesso.\n",
      "Corpus do dia 14 salvo com sucesso.\n",
      "Corpus do dia 15 salvo com sucesso.\n",
      "Corpus do dia 16 salvo com sucesso.\n",
      "Corpus do dia 17 salvo com sucesso.\n",
      "Corpus do dia 18 salvo com sucesso.\n",
      "Corpus do dia 19 salvo com sucesso.\n",
      "Corpus do dia 20 salvo com sucesso.\n",
      "Corpus do dia 21 salvo com sucesso.\n",
      "Corpus do dia 22 salvo com sucesso.\n",
      "Corpus do dia 23 salvo com sucesso.\n",
      "Corpus do dia 24 salvo com sucesso.\n",
      "Corpus do dia 25 salvo com sucesso.\n",
      "Corpus do dia 26 salvo com sucesso.\n",
      "Corpus do dia 27 salvo com sucesso.\n",
      "Corpus do dia 28 salvo com sucesso.\n",
      "Corpus do dia 29 salvo com sucesso.\n",
      "Corpus do dia 30 salvo com sucesso.\n",
      "Corpus do dia 31 salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.preprocessing import *\n",
    "from src.preprocessing.nlp import *\n",
    "\n",
    "# Defini√ß√£o de par√¢metros para filtragem e an√°lise\n",
    "FILTER = 5      # N√∫mero m√≠nimo de pacotes para considerar\n",
    "TOP_PORTS = 2500  # Quantidade de portas analisadas\n",
    "\n",
    "# Carregamento dos dados de tr√°fego da rede\n",
    "print(\"Carregando dados de tr√°fego...\")\n",
    "df_trafego = pd.read_csv(\"trafego_rede_simulado.csv\")\n",
    "print(df_trafego.head())\n",
    "\n",
    "# Aplica√ß√£o do filtro de pacotes antes da renomea√ß√£o de colunas\n",
    "df_trafego = apply_packets_filter(df_trafego, FILTER)\n",
    "\n",
    "# Renomea√ß√£o de colunas para padroniza√ß√£o\n",
    "df_trafego.rename(columns={'dst_ip': 'destino', 'pacotes': 'pkts'}, inplace=True)\n",
    "\n",
    "processed_df = [df_trafego]\n",
    "\n",
    "# Carregamento do Ground Truth\n",
    "print(\"Carregando dados de Ground Truth...\")\n",
    "gt = pd.read_csv(\"ground_truth_simulado.csv\")\n",
    "\n",
    "# Gera√ß√£o de estat√≠sticas gerais\n",
    "print(\"Calculando estat√≠sticas gerais...\")\n",
    "df_merged = pd.concat(processed_df, ignore_index=True)[['src_ip', 'destino', 'pkts']]\n",
    "df_merged = df_merged.merge(gt, on='src_ip', how='left').dropna()\n",
    "\n",
    "print(df_merged.groupby('label').agg({\n",
    "    'src_ip': lambda x: len(set(x)),  # N√∫mero de IPs √∫nicos\n",
    "    'destino': lambda x: len(set(x)),  # N√∫mero de destinos √∫nicos\n",
    "    'pkts': sum  # Soma total de pacotes\n",
    "}))\n",
    "\n",
    "# Carregamento da matriz de adjac√™ncia do grafo da rede\n",
    "print(\"Carregando matriz de adjac√™ncia...\")\n",
    "df_grafo = pd.read_csv(\"matriz_adjacencia_simulada.txt\", sep=\" \", names=[\"origem\", \"destino\", \"peso\"])\n",
    "\n",
    "# C√°lculo de estat√≠sticas do grafo\n",
    "total_nos = pd.concat([df_grafo['origem'], df_grafo['destino']]).nunique()\n",
    "total_arestas = df_grafo.shape[0]\n",
    "\n",
    "print(f\"Total de N√≥s (IPs distintos): {total_nos}\")\n",
    "print(f\"Total de Arestas (Conex√µes na Rede): {total_arestas}\")\n",
    "\n",
    "# Carregamento de dicion√°rios de lookup\n",
    "print(\"Carregando dicion√°rios de lookup...\")\n",
    "with open(\"../data/graph/ip_lookup.json\", \"r\") as file:\n",
    "    ip_lookup = json.load(file)\n",
    "\n",
    "with open(\"../data/graph/port_lookup.json\", \"r\") as file:\n",
    "    port_lookup = json.load(file)\n",
    "\n",
    "# Extra√ß√£o de features dos n√≥s com base nos dicion√°rios de lookup\n",
    "print(\"Extraindo features dos n√≥s...\")\n",
    "df_merged['origem_id'] = df_merged['src_ip'].map(ip_lookup)\n",
    "df_merged['destino_id'] = df_merged['destino'].map(ip_lookup)\n",
    "\n",
    "node_features = df_merged.groupby('origem_id').agg({'pkts': ['mean', 'sum']}).reset_index()\n",
    "node_features.columns = ['origem_id', 'media_pkts', 'soma_pkts']\n",
    "\n",
    "print(\"Features extra√≠das com sucesso:\")\n",
    "print(node_features.head())\n",
    "\n",
    "# Salvamento dos dados processados\n",
    "os.makedirs(\"../data/features\", exist_ok=True)\n",
    "for day in df_trafego['interval'].unique():\n",
    "    snapshot = df_trafego[df_trafego['interval'] == day]\n",
    "    snapshot['origem_id'] = snapshot['src_ip'].map(ip_lookup)\n",
    "    snapshot['destino_id'] = snapshot['destino'].map(ip_lookup)\n",
    "    node_features_day = snapshot.groupby('origem_id').agg({'pkts': ['mean', 'sum']}).reset_index()\n",
    "    node_features_day.columns = ['origem_id', 'media_pkts', 'soma_pkts']\n",
    "    node_features_day.to_csv(f\"../data/features/features_{day}.csv\", index=False)\n",
    "    print(f\"Features do dia {day} salvas em '../data/features/features_{day}.csv'.\")\n",
    "\n",
    "print(\"Extra√ß√£o e salvamento de features conclu√≠dos! üöÄ\")\n",
    "\n",
    "# An√°lise de tr√°fego para NLP\n",
    "print(\"Analisando tr√°fego para NLP...\")\n",
    "raw_df = df_trafego.copy()\n",
    "\n",
    "# Gera√ß√£o do corpus NLP\n",
    "print(\"Gerando corpus NLP...\")\n",
    "tot_intervals = sorted(raw_df['interval'].unique())\n",
    "\n",
    "for day in tqdm(tot_intervals):\n",
    "    # Filtragem do tr√°fego do dia\n",
    "    snapshot = raw_df[raw_df['interval'] == day].sort_values('ts')\n",
    "    # Agrupamento de IPs de origem por porta de destino\n",
    "    corpus = snapshot.groupby('dst_port')['src_ip'].apply(list).to_dict()\n",
    "    # Ordena√ß√£o das portas e alinhamento do corpus\n",
    "    port_order = list(corpus.keys())\n",
    "    corpus_list = [corpus[port] for port in port_order]\n",
    "    # Criar diret√≥rio de sa√≠da\n",
    "    os.makedirs(\"../data/corpus\", exist_ok=True)\n",
    "    # Salvamento do corpus NLP\n",
    "    with open(f'../data/corpus/corpus_{day}.pkl', 'wb') as file:\n",
    "        pickle.dump({'ports': port_order, 'corpus': corpus_list}, file)\n",
    "    print(f\"Corpus do dia {day} salvo com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gera√ß√£o de embeddings (NLP) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ea48fffcb4ddab74b2c1f8831950f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando modelo para o dia 1...\n",
      "Embeddings do dia 1 salvos em '../data/nlp_embeddings/embeddings_idarkvec_1.csv'\n",
      "Atualizando modelo para o dia 10...\n",
      "Embeddings do dia 10 salvos em '../data/nlp_embeddings/embeddings_idarkvec_10.csv'\n",
      "Atualizando modelo para o dia 11...\n",
      "Embeddings do dia 11 salvos em '../data/nlp_embeddings/embeddings_idarkvec_11.csv'\n",
      "Atualizando modelo para o dia 12...\n",
      "Embeddings do dia 12 salvos em '../data/nlp_embeddings/embeddings_idarkvec_12.csv'\n",
      "Atualizando modelo para o dia 13...\n",
      "Embeddings do dia 13 salvos em '../data/nlp_embeddings/embeddings_idarkvec_13.csv'\n",
      "Atualizando modelo para o dia 14...\n",
      "Embeddings do dia 14 salvos em '../data/nlp_embeddings/embeddings_idarkvec_14.csv'\n",
      "Atualizando modelo para o dia 15...\n",
      "Embeddings do dia 15 salvos em '../data/nlp_embeddings/embeddings_idarkvec_15.csv'\n",
      "Atualizando modelo para o dia 16...\n",
      "Embeddings do dia 16 salvos em '../data/nlp_embeddings/embeddings_idarkvec_16.csv'\n",
      "Atualizando modelo para o dia 17...\n",
      "Embeddings do dia 17 salvos em '../data/nlp_embeddings/embeddings_idarkvec_17.csv'\n",
      "Atualizando modelo para o dia 18...\n",
      "Embeddings do dia 18 salvos em '../data/nlp_embeddings/embeddings_idarkvec_18.csv'\n",
      "Atualizando modelo para o dia 19...\n",
      "Embeddings do dia 19 salvos em '../data/nlp_embeddings/embeddings_idarkvec_19.csv'\n",
      "Atualizando modelo para o dia 2...\n",
      "Embeddings do dia 2 salvos em '../data/nlp_embeddings/embeddings_idarkvec_2.csv'\n",
      "Atualizando modelo para o dia 20...\n",
      "Embeddings do dia 20 salvos em '../data/nlp_embeddings/embeddings_idarkvec_20.csv'\n",
      "Atualizando modelo para o dia 21...\n",
      "Embeddings do dia 21 salvos em '../data/nlp_embeddings/embeddings_idarkvec_21.csv'\n",
      "Atualizando modelo para o dia 22...\n",
      "Embeddings do dia 22 salvos em '../data/nlp_embeddings/embeddings_idarkvec_22.csv'\n",
      "Atualizando modelo para o dia 23...\n",
      "Embeddings do dia 23 salvos em '../data/nlp_embeddings/embeddings_idarkvec_23.csv'\n",
      "Atualizando modelo para o dia 24...\n",
      "Embeddings do dia 24 salvos em '../data/nlp_embeddings/embeddings_idarkvec_24.csv'\n",
      "Atualizando modelo para o dia 25...\n",
      "Embeddings do dia 25 salvos em '../data/nlp_embeddings/embeddings_idarkvec_25.csv'\n",
      "Atualizando modelo para o dia 26...\n",
      "Embeddings do dia 26 salvos em '../data/nlp_embeddings/embeddings_idarkvec_26.csv'\n",
      "Atualizando modelo para o dia 27...\n",
      "Embeddings do dia 27 salvos em '../data/nlp_embeddings/embeddings_idarkvec_27.csv'\n",
      "Atualizando modelo para o dia 28...\n",
      "Embeddings do dia 28 salvos em '../data/nlp_embeddings/embeddings_idarkvec_28.csv'\n",
      "Atualizando modelo para o dia 29...\n",
      "Embeddings do dia 29 salvos em '../data/nlp_embeddings/embeddings_idarkvec_29.csv'\n",
      "Atualizando modelo para o dia 3...\n",
      "Embeddings do dia 3 salvos em '../data/nlp_embeddings/embeddings_idarkvec_3.csv'\n",
      "Atualizando modelo para o dia 30...\n",
      "Embeddings do dia 30 salvos em '../data/nlp_embeddings/embeddings_idarkvec_30.csv'\n",
      "Atualizando modelo para o dia 31...\n",
      "Embeddings do dia 31 salvos em '../data/nlp_embeddings/embeddings_idarkvec_31.csv'\n",
      "Atualizando modelo para o dia 4...\n",
      "Embeddings do dia 4 salvos em '../data/nlp_embeddings/embeddings_idarkvec_4.csv'\n",
      "Atualizando modelo para o dia 5...\n",
      "Embeddings do dia 5 salvos em '../data/nlp_embeddings/embeddings_idarkvec_5.csv'\n",
      "Atualizando modelo para o dia 6...\n",
      "Embeddings do dia 6 salvos em '../data/nlp_embeddings/embeddings_idarkvec_6.csv'\n",
      "Atualizando modelo para o dia 7...\n",
      "Embeddings do dia 7 salvos em '../data/nlp_embeddings/embeddings_idarkvec_7.csv'\n",
      "Atualizando modelo para o dia 8...\n",
      "Embeddings do dia 8 salvos em '../data/nlp_embeddings/embeddings_idarkvec_8.csv'\n",
      "Atualizando modelo para o dia 9...\n",
      "Embeddings do dia 9 salvos em '../data/nlp_embeddings/embeddings_idarkvec_9.csv'\n"
     ]
    }
   ],
   "source": [
    "from src.models.nlp import iWord2Vec\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Inicializa o modelo\n",
    "word2vec = iWord2Vec(c=5, e=128, epochs=1, seed=15)\n",
    "\n",
    "# Processa os arquivos do corpus\n",
    "for file in tqdm(sorted(glob.glob('../data/corpus/corpus_*.pkl'))):\n",
    "    try:\n",
    "        # Obt√©m o dia a partir do nome do arquivo\n",
    "        day = file.split('/')[-1].replace('corpus_', '').replace('.pkl', '')\n",
    "        \n",
    "        # Valida se o dia √© um n√∫mero entre 1 e 31\n",
    "        try:\n",
    "            day_int = int(day)\n",
    "            if not (1 <= day_int <= 31):\n",
    "                print(f\"Ignorando {file}: dia {day_int} inv√°lido.\")\n",
    "                continue\n",
    "        except ValueError:\n",
    "            print(f\"Ignorando {file}: nome do arquivo n√£o cont√©m um dia v√°lido.\")\n",
    "            continue\n",
    "        \n",
    "        # Carrega o corpus\n",
    "        with open(file, 'rb') as f:\n",
    "            corpus_data = pickle.load(f)\n",
    "        \n",
    "        # Verifica a estrutura esperada\n",
    "        if 'ports' not in corpus_data or 'corpus' not in corpus_data:\n",
    "            print(f\"Estrutura inv√°lida em {file}. Ignorando.\")\n",
    "            continue\n",
    "        \n",
    "        port_order = corpus_data['ports']\n",
    "        corpus = corpus_data['corpus']\n",
    "        \n",
    "        # Verifica se o corpus est√° vazio\n",
    "        if not corpus:\n",
    "            print(f\"Corpus vazio em {file}. Ignorando.\")\n",
    "            continue\n",
    "        \n",
    "        # Treina ou atualiza o modelo conforme o dia\n",
    "        if day_int == 1:\n",
    "            print(f\"Treinando modelo para o dia {day_int}...\")\n",
    "            word2vec.train(corpus)\n",
    "        else:\n",
    "            print(f\"Atualizando modelo para o dia {day_int}...\")\n",
    "            word2vec.update(corpus)\n",
    "        \n",
    "        # Obt√©m os embeddings\n",
    "        embeddings = word2vec.get_embeddings()\n",
    "        if isinstance(embeddings, pd.DataFrame):\n",
    "            embeddings.index.name = \"src_ip\"\n",
    "            embeddings = embeddings.reset_index()\n",
    "        else:\n",
    "            embeddings = pd.DataFrame(embeddings)\n",
    "            embeddings.insert(0, \"src_ip\", port_order)\n",
    "        \n",
    "        # Salva os embeddings em CSV\n",
    "        os.makedirs(\"../data/nlp_embeddings\", exist_ok=True)\n",
    "        embeddings.to_csv(f'../data/nlp_embeddings/embeddings_idarkvec_{day}.csv', index=False)\n",
    "        \n",
    "        print(f\"Embeddings do dia {day} salvos em '../data/nlp_embeddings/embeddings_idarkvec_{day}.csv'\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifica√ß√£o (NLP) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificando conte√∫do do arquivo:\n",
      "         src_ip         0         1         2         3         4         5  \\\n",
      "0  192.168.1.26  0.006738  0.003012  0.003212  0.004935 -0.004036 -0.002431   \n",
      "1  192.168.1.33  0.002965 -0.003778  0.000793 -0.001849 -0.000688 -0.006697   \n",
      "2  192.168.1.40 -0.001823  0.007714  0.007047 -0.003224  0.007148  0.004393   \n",
      "3  192.168.1.38 -0.005350  0.004542 -0.001045  0.005886 -0.000692 -0.002025   \n",
      "4   192.168.1.9 -0.002121 -0.001018 -0.006599 -0.000633 -0.002287  0.005048   \n",
      "\n",
      "          6         7         8  ...       118       119       120       121  \\\n",
      "0 -0.004646 -0.007112 -0.000888  ... -0.006956  0.003519  0.003514  0.004902   \n",
      "1 -0.006737  0.003017 -0.004178  ... -0.004152  0.002504 -0.000645  0.005921   \n",
      "2 -0.002487 -0.000087  0.002947  ...  0.004844  0.000537  0.001595  0.000475   \n",
      "3  0.004510  0.002709 -0.003699  ... -0.000646 -0.004903 -0.001364 -0.001819   \n",
      "4  0.000739 -0.003050 -0.006083  ... -0.003366 -0.004525 -0.001277 -0.005287   \n",
      "\n",
      "        122       123       124       125       126       127  \n",
      "0 -0.005652  0.007779  0.000233 -0.000664 -0.002660 -0.004676  \n",
      "1 -0.004516  0.002195  0.006136  0.000307  0.001234 -0.002527  \n",
      "2 -0.002731 -0.000327 -0.002062  0.000400 -0.000519  0.004494  \n",
      "3  0.002665  0.006902  0.001184 -0.002025  0.001991  0.004633  \n",
      "4 -0.007148  0.006845  0.005068 -0.004754 -0.004283 -0.006139  \n",
      "\n",
      "[5 rows x 129 columns]\n",
      "\n",
      "Verificando estrutura dos embeddings...\n",
      "         src_ip         0         1         2         3         4         5  \\\n",
      "0  192.168.1.26  0.006738  0.003012  0.003212  0.004935 -0.004036 -0.002431   \n",
      "1  192.168.1.33  0.002965 -0.003778  0.000793 -0.001849 -0.000688 -0.006697   \n",
      "2  192.168.1.40 -0.001823  0.007714  0.007047 -0.003224  0.007148  0.004393   \n",
      "3  192.168.1.38 -0.005350  0.004542 -0.001045  0.005886 -0.000692 -0.002025   \n",
      "4   192.168.1.9 -0.002121 -0.001018 -0.006599 -0.000633 -0.002287  0.005048   \n",
      "\n",
      "          6         7         8  ...       118       119       120       121  \\\n",
      "0 -0.004646 -0.007112 -0.000888  ... -0.006956  0.003519  0.003514  0.004902   \n",
      "1 -0.006737  0.003017 -0.004178  ... -0.004152  0.002504 -0.000645  0.005921   \n",
      "2 -0.002487 -0.000087  0.002947  ...  0.004844  0.000537  0.001595  0.000475   \n",
      "3  0.004510  0.002709 -0.003699  ... -0.000646 -0.004903 -0.001364 -0.001819   \n",
      "4  0.000739 -0.003050 -0.006083  ... -0.003366 -0.004525 -0.001277 -0.005287   \n",
      "\n",
      "        122       123       124       125       126       127  \n",
      "0 -0.005652  0.007779  0.000233 -0.000664 -0.002660 -0.004676  \n",
      "1 -0.004516  0.002195  0.006136  0.000307  0.001234 -0.002527  \n",
      "2 -0.002731 -0.000327 -0.002062  0.000400 -0.000519  0.004494  \n",
      "3  0.002665  0.006902  0.001184 -0.002025  0.001991  0.004633  \n",
      "4 -0.007148  0.006845  0.005068 -0.004754 -0.004283 -0.006139  \n",
      "\n",
      "[5 rows x 129 columns]\n",
      "\n",
      "Distribui√ß√£o de r√≥tulos antes da filtragem:\n",
      "label\n",
      "unknown            7\n",
      "mirai              2\n",
      "benign             1\n",
      "unk_bruteforcer    1\n",
      "securitytrails     1\n",
      "internetcensus     1\n",
      "censys             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribui√ß√£o de r√≥tulos ap√≥s a filtragem:\n",
      "label\n",
      "mirai              2\n",
      "benign             1\n",
      "unk_bruteforcer    1\n",
      "securitytrails     1\n",
      "internetcensus     1\n",
      "censys             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Relat√≥rio de Classifica√ß√£o:\n",
      "                 precision    recall  f1-score   support\n",
      "benign            0.500000  1.000000  0.666667  1.000000\n",
      "censys            0.000000  0.000000  0.000000  1.000000\n",
      "internetcensus    0.000000  0.000000  0.000000  1.000000\n",
      "mirai             0.000000  0.000000  0.000000  2.000000\n",
      "securitytrails    0.000000  0.000000  0.000000  1.000000\n",
      "unk_bruteforcer   0.000000  0.000000  0.000000  1.000000\n",
      "accuracy          0.142857  0.142857  0.142857  0.142857\n",
      "macro avg         0.083333  0.166667  0.111111  7.000000\n",
      "weighted avg      0.071429  0.142857  0.095238  7.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Carrega o arquivo de ground truth contendo os r√≥tulos reais\n",
    "gt_file = 'ground_truth_simulado.csv'\n",
    "if not os.path.exists(gt_file):\n",
    "    raise FileNotFoundError(f\"Arquivo de ground truth {gt_file} n√£o encontrado.\")\n",
    "\n",
    "gt = pd.read_csv(gt_file)\n",
    "\n",
    "gt['src_ip'] = gt['src_ip'].astype(str)  # Garante que os endere√ßos IP sejam tratados como strings\n",
    "\n",
    "if 'src_ip' not in gt.columns or 'label' not in gt.columns:\n",
    "    raise ValueError(\"O ground truth deve conter as colunas 'src_ip' e 'label'.\")\n",
    "\n",
    "# Implementa√ß√£o de um classificador k-NN personalizado com vizinhos adaptativos\n",
    "class KnnClassifier:\n",
    "    def __init__(self, n_neighbors=3, metric='cosine'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "        self.model = None  # O modelo ser√° inicializado na chamada do m√©todo `fit`\n",
    "\n",
    "    def fit(self, X_train, y_train, scale_data=False):\n",
    "        num_samples = len(y_train)\n",
    "\n",
    "        k = min(self.n_neighbors, num_samples)  # Ajusta `k` para n√£o ser maior que o n√∫mero de amostras dispon√≠veis\n",
    "        if k < 1:\n",
    "            raise ValueError(\"N√∫mero insuficiente de amostras para classifica√ß√£o k-NN.\")\n",
    "\n",
    "        self.model = KNeighborsClassifier(n_neighbors=k, metric=self.metric)\n",
    "\n",
    "        if scale_data:\n",
    "            self.scaler = StandardScaler()\n",
    "            X_train = self.scaler.fit_transform(X_train)  # Normaliza os dados antes do treinamento\n",
    "\n",
    "        self.model.fit(X_train, y_train)  # Treina o modelo com os dados fornecidos\n",
    "\n",
    "    def predict(self, X_test, scale_data=False):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"O modelo ainda n√£o foi treinado.\")\n",
    "\n",
    "        if scale_data:\n",
    "            X_test = self.scaler.transform(X_test)  # Normaliza os dados antes da predi√ß√£o\n",
    "\n",
    "        return self.model.predict(X_test)  # Retorna as previs√µes do modelo\n",
    "\n",
    "# Pipeline de classifica√ß√£o\n",
    "def classification_pipeline(embeddings, gt):\n",
    "    print(\"\\nVerificando estrutura dos embeddings...\")\n",
    "    print(embeddings.head())  # Exibe amostras dos embeddings para verifica√ß√£o\n",
    "\n",
    "    if 'src_ip' not in embeddings.columns:\n",
    "        raise ValueError(\"Arquivo de embeddings n√£o cont√©m 'src_ip'. Verifique o formato.\")\n",
    "\n",
    "    embeddings['src_ip'] = embeddings['src_ip'].astype(str)  # Garante que os endere√ßos IP sejam strings\n",
    "\n",
    "    # Mescla os embeddings com o ground truth com base no campo 'src_ip'\n",
    "    embeddings = embeddings.merge(gt, on='src_ip', how='left').fillna('unknown')\n",
    "\n",
    "    print(\"\\nDistribui√ß√£o de r√≥tulos antes da filtragem:\")\n",
    "    print(embeddings['label'].value_counts())  # Mostra a distribui√ß√£o dos r√≥tulos antes da filtragem\n",
    "\n",
    "    valid_labels = embeddings['label'].value_counts().index\n",
    "    embeddings.loc[~embeddings['label'].isin(valid_labels), 'label'] = 'unknown'\n",
    "\n",
    "    # Remove registros com r√≥tulo 'unknown' apenas se houver outras classes v√°lidas\n",
    "    if (embeddings['label'] == 'unknown').sum() != len(embeddings):\n",
    "        embeddings = embeddings[embeddings['label'] != 'unknown']\n",
    "\n",
    "    print(\"\\nDistribui√ß√£o de r√≥tulos ap√≥s a filtragem:\")\n",
    "    print(embeddings['label'].value_counts())  # Exibe a distribui√ß√£o dos r√≥tulos ap√≥s a filtragem\n",
    "\n",
    "    # Verifica se h√° classes suficientes para treinar o modelo\n",
    "    if len(np.unique(embeddings['label'].values)) < 2:\n",
    "        raise ValueError(\"\\nErro: quantidade insuficiente de classes v√°lidas para classifica√ß√£o. Verifique a distribui√ß√£o de r√≥tulos.\")\n",
    "\n",
    "    X_train = embeddings.drop(columns=['label', 'src_ip'], errors='ignore').values  # Remove colunas n√£o num√©ricas\n",
    "    y_train = np.ravel(embeddings[['label']].values)  # Obt√©m os r√≥tulos para treinamento\n",
    "\n",
    "    knn = KnnClassifier(n_neighbors=3, metric='cosine')\n",
    "    knn.fit(X_train, y_train, scale_data=True)  # Treina o classificador\n",
    "\n",
    "    valid_indices = np.arange(len(y_train)).astype(int).flatten()\n",
    "\n",
    "    y_true = y_train[valid_indices]  # Obt√©m os r√≥tulos verdadeiros\n",
    "    y_pred = knn.predict(X_train[valid_indices], scale_data=True)  # Obt√©m as previs√µes do modelo\n",
    "\n",
    "    crep = classification_report(y_true, y_pred, labels=np.unique(y_true), output_dict=True)  # Gera o relat√≥rio de classifica√ß√£o\n",
    "    return crep\n",
    "\n",
    "# Carrega o arquivo de embeddings e executa a classifica√ß√£o\n",
    "embedding_file = '../data/nlp_embeddings/embeddings_idarkvec_2.csv'\n",
    "if not os.path.exists(embedding_file):\n",
    "    raise FileNotFoundError(f\"Arquivo de embeddings {embedding_file} n√£o encontrado.\")\n",
    "\n",
    "embeddings = pd.read_csv(embedding_file)\n",
    "\n",
    "print(\"\\nVerificando conte√∫do do arquivo:\")\n",
    "print(embeddings.head())  # Exibe amostras do arquivo de embeddings para depura√ß√£o\n",
    "\n",
    "if 'src_ip' not in embeddings.columns:\n",
    "    raise KeyError(\"A coluna 'src_ip' est√° ausente no arquivo de embeddings. Verifique o formato dos dados.\")\n",
    "\n",
    "embeddings['src_ip'] = embeddings['src_ip'].astype(str)  # Converte os IPs para string para garantir consist√™ncia\n",
    "\n",
    "classification_report = classification_pipeline(embeddings, gt)  # Executa o pipeline de classifica√ß√£o\n",
    "\n",
    "print(\"\\nRelat√≥rio de Classifica√ß√£o:\")\n",
    "print(pd.DataFrame(classification_report).transpose())  # Exibe o relat√≥rio de classifica√ß√£o final\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
