{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria√ß√£o arquivos lookup ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados de tr√°fego...\n",
      "Arquivos ip_lookup.json e port_lookup.json recriados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Carregamento dos Dados de Tr√°fego\n",
    "print(\"Carregando dados de tr√°fego...\")\n",
    "df_trafego = pd.read_csv(\"trafego_rede_simulado.csv\")\n",
    "\n",
    "# Criar diret√≥rio se n√£o existir\n",
    "os.makedirs(\"../data/graph\", exist_ok=True)\n",
    "\n",
    "# Gerar lookup para IPs √∫nicos\n",
    "unique_ips = pd.concat([df_trafego['src_ip'], df_trafego['dst_ip']]).unique()\n",
    "ip_lookup = {str(ip): idx for idx, ip in enumerate(unique_ips)}\n",
    "\n",
    "# Gerar lookup para portas √∫nicas\n",
    "unique_ports = df_trafego['dst_port'].unique()\n",
    "port_lookup = {str(port): idx for idx, port in enumerate(unique_ports)}\n",
    "\n",
    "# Salvar os dicion√°rios como JSON\n",
    "with open(\"../data/graph/ip_lookup.json\", \"w\") as file:\n",
    "    json.dump(ip_lookup, file, indent=4)\n",
    "\n",
    "with open(\"../data/graph/port_lookup.json\", \"w\") as file:\n",
    "    json.dump(port_lookup, file, indent=4)\n",
    "\n",
    "print(\"Arquivos ip_lookup.json e port_lookup.json recriados com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria√ß√£o do corpus e das features ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados de tr√°fego...\n",
      "         src_ip        dst_ip  pacotes  bytes_transferidos protocolo  \\\n",
      "0   192.168.1.3  192.168.1.27      981               51284       UDP   \n",
      "1  192.168.1.33  192.168.1.34      476               62016      ICMP   \n",
      "2   192.168.1.6  192.168.1.27      702               74072       TCP   \n",
      "3  192.168.1.50  192.168.1.17      634               62287       UDP   \n",
      "4  192.168.1.10  192.168.1.43      558               13084       TCP   \n",
      "\n",
      "   interval   ts  dst_port  \n",
      "0        14  1.0     15579  \n",
      "1        18  2.0     12631  \n",
      "2        18  3.0     39440  \n",
      "3        29  4.0      6357  \n",
      "4         7  5.0     37372  \n",
      "Carregando dados de Ground Truth...\n",
      "Calculando estat√≠sticas gerais...\n",
      "                 src_ip  destino  pkts\n",
      "label                                 \n",
      "benign                1        6  2740\n",
      "censys                1        6  2968\n",
      "internetcensus        1        6  3473\n",
      "mirai                 2       11  6019\n",
      "securitytrails        1        4  4066\n",
      "unk_bruteforcer       1        6  4094\n",
      "Carregando matriz de adjac√™ncia...\n",
      "Total de N√≥s (IPs distintos): 50\n",
      "Total de Arestas (Conex√µes na Rede): 191\n",
      "Carregando dicion√°rios de lookup...\n",
      "Extraindo features dos n√≥s...\n",
      "Features extra√≠das com sucesso:\n",
      "   origem_id  media_pkts  soma_pkts\n",
      "0          4  554.166667       3325\n",
      "1          5  384.857143       2694\n",
      "2          8  578.833333       3473\n",
      "3         14  456.666667       2740\n",
      "4         16  677.666667       4066\n",
      "Features do dia 18 salvas em '../data/features/features_18.csv'.\n",
      "Features do dia 7 salvas em '../data/features/features_7.csv'.\n",
      "Features do dia 17 salvas em '../data/features/features_17.csv'.\n",
      "Features do dia 22 salvas em '../data/features/features_22.csv'.\n",
      "Features do dia 2 salvas em '../data/features/features_2.csv'.\n",
      "Features do dia 16 salvas em '../data/features/features_16.csv'.\n",
      "Features do dia 6 salvas em '../data/features/features_6.csv'.\n",
      "Features do dia 26 salvas em '../data/features/features_26.csv'.\n",
      "Features do dia 28 salvas em '../data/features/features_28.csv'.\n",
      "Features do dia 24 salvas em '../data/features/features_24.csv'.\n",
      "Features do dia 29 salvas em '../data/features/features_29.csv'.\n",
      "Features do dia 31 salvas em '../data/features/features_31.csv'.\n",
      "Features do dia 13 salvas em '../data/features/features_13.csv'.\n",
      "Features do dia 9 salvas em '../data/features/features_9.csv'.\n",
      "Features do dia 3 salvas em '../data/features/features_3.csv'.\n",
      "Features do dia 21 salvas em '../data/features/features_21.csv'.\n",
      "Features do dia 19 salvas em '../data/features/features_19.csv'.\n",
      "Features do dia 10 salvas em '../data/features/features_10.csv'.\n",
      "Features do dia 30 salvas em '../data/features/features_30.csv'.\n",
      "Features do dia 14 salvas em '../data/features/features_14.csv'.\n",
      "Features do dia 20 salvas em '../data/features/features_20.csv'.\n",
      "Features do dia 1 salvas em '../data/features/features_1.csv'.\n",
      "Features do dia 4 salvas em '../data/features/features_4.csv'.\n",
      "Features do dia 11 salvas em '../data/features/features_11.csv'.\n",
      "Features do dia 5 salvas em '../data/features/features_5.csv'.\n",
      "Features do dia 15 salvas em '../data/features/features_15.csv'.\n",
      "Features do dia 27 salvas em '../data/features/features_27.csv'.\n",
      "Features do dia 8 salvas em '../data/features/features_8.csv'.\n",
      "Features do dia 23 salvas em '../data/features/features_23.csv'.\n",
      "Features do dia 25 salvas em '../data/features/features_25.csv'.\n",
      "Features do dia 12 salvas em '../data/features/features_12.csv'.\n",
      "Extra√ß√£o e salvamento de features conclu√≠dos! üöÄ\n",
      "Analisando tr√°fego para NLP...\n",
      "Gerando corpus NLP...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5391fe31d91943e68c846a7531f5b494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus do dia 1 salvo com sucesso.\n",
      "Corpus do dia 2 salvo com sucesso.\n",
      "Corpus do dia 3 salvo com sucesso.\n",
      "Corpus do dia 4 salvo com sucesso.\n",
      "Corpus do dia 5 salvo com sucesso.\n",
      "Corpus do dia 6 salvo com sucesso.\n",
      "Corpus do dia 7 salvo com sucesso.\n",
      "Corpus do dia 8 salvo com sucesso.\n",
      "Corpus do dia 9 salvo com sucesso.\n",
      "Corpus do dia 10 salvo com sucesso.\n",
      "Corpus do dia 11 salvo com sucesso.\n",
      "Corpus do dia 12 salvo com sucesso.\n",
      "Corpus do dia 13 salvo com sucesso.\n",
      "Corpus do dia 14 salvo com sucesso.\n",
      "Corpus do dia 15 salvo com sucesso.\n",
      "Corpus do dia 16 salvo com sucesso.\n",
      "Corpus do dia 17 salvo com sucesso.\n",
      "Corpus do dia 18 salvo com sucesso.\n",
      "Corpus do dia 19 salvo com sucesso.\n",
      "Corpus do dia 20 salvo com sucesso.\n",
      "Corpus do dia 21 salvo com sucesso.\n",
      "Corpus do dia 22 salvo com sucesso.\n",
      "Corpus do dia 23 salvo com sucesso.\n",
      "Corpus do dia 24 salvo com sucesso.\n",
      "Corpus do dia 25 salvo com sucesso.\n",
      "Corpus do dia 26 salvo com sucesso.\n",
      "Corpus do dia 27 salvo com sucesso.\n",
      "Corpus do dia 28 salvo com sucesso.\n",
      "Corpus do dia 29 salvo com sucesso.\n",
      "Corpus do dia 30 salvo com sucesso.\n",
      "Corpus do dia 31 salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.preprocessing import *\n",
    "from src.preprocessing.nlp import *\n",
    "\n",
    "# Defini√ß√£o de par√¢metros para filtragem e an√°lise\n",
    "FILTER = 5      # N√∫mero m√≠nimo de pacotes para considerar\n",
    "\n",
    "# Carregamento dos dados de tr√°fego da rede\n",
    "print(\"Carregando dados de tr√°fego...\")\n",
    "df_trafego = pd.read_csv(\"trafego_rede_simulado.csv\")\n",
    "print(df_trafego.head())\n",
    "\n",
    "# Aplica√ß√£o do filtro de pacotes antes da renomea√ß√£o de colunas\n",
    "df_trafego = apply_packets_filter(df_trafego, FILTER)\n",
    "\n",
    "# Renomea√ß√£o de colunas para padroniza√ß√£o\n",
    "df_trafego.rename(columns={'dst_ip': 'destino', 'pacotes': 'pkts'}, inplace=True)\n",
    "\n",
    "processed_df = [df_trafego]\n",
    "\n",
    "# Carregamento do Ground Truth\n",
    "print(\"Carregando dados de Ground Truth...\")\n",
    "gt = pd.read_csv(\"ground_truth_simulado.csv\")\n",
    "\n",
    "# Gera√ß√£o de estat√≠sticas gerais\n",
    "print(\"Calculando estat√≠sticas gerais...\")\n",
    "df_merged = pd.concat(processed_df, ignore_index=True)[['src_ip', 'destino', 'pkts']]\n",
    "df_merged = df_merged.merge(gt, on='src_ip', how='left').dropna()\n",
    "\n",
    "print(df_merged.groupby('label').agg({\n",
    "    'src_ip': lambda x: len(set(x)),  # N√∫mero de IPs √∫nicos\n",
    "    'destino': lambda x: len(set(x)),  # N√∫mero de destinos √∫nicos\n",
    "    'pkts': sum  # Soma total de pacotes\n",
    "}))\n",
    "\n",
    "# Carregamento da matriz de adjac√™ncia do grafo da rede\n",
    "print(\"Carregando matriz de adjac√™ncia...\")\n",
    "df_grafo = pd.read_csv(\"matriz_adjacencia_simulada.txt\", sep=\" \", names=[\"origem\", \"destino\", \"peso\"])\n",
    "\n",
    "# C√°lculo de estat√≠sticas do grafo\n",
    "total_nos = pd.concat([df_grafo['origem'], df_grafo['destino']]).nunique()\n",
    "total_arestas = df_grafo.shape[0]\n",
    "\n",
    "print(f\"Total de N√≥s (IPs distintos): {total_nos}\")\n",
    "print(f\"Total de Arestas (Conex√µes na Rede): {total_arestas}\")\n",
    "\n",
    "# Carregamento de dicion√°rios de lookup\n",
    "print(\"Carregando dicion√°rios de lookup...\")\n",
    "with open(\"../data/graph/ip_lookup.json\", \"r\") as file:\n",
    "    ip_lookup = json.load(file)\n",
    "\n",
    "with open(\"../data/graph/port_lookup.json\", \"r\") as file:\n",
    "    port_lookup = json.load(file)\n",
    "\n",
    "# Extra√ß√£o de features dos n√≥s com base nos dicion√°rios de lookup\n",
    "print(\"Extraindo features dos n√≥s...\")\n",
    "df_merged['origem_id'] = df_merged['src_ip'].map(ip_lookup)\n",
    "df_merged['destino_id'] = df_merged['destino'].map(ip_lookup)\n",
    "\n",
    "node_features = df_merged.groupby('origem_id').agg({'pkts': ['mean', 'sum']}).reset_index()\n",
    "node_features.columns = ['origem_id', 'media_pkts', 'soma_pkts']\n",
    "\n",
    "print(\"Features extra√≠das com sucesso:\")\n",
    "print(node_features.head())\n",
    "\n",
    "# Salvamento dos dados processados\n",
    "os.makedirs(\"../data/features\", exist_ok=True)\n",
    "for day in df_trafego['interval'].unique():\n",
    "    snapshot = df_trafego[df_trafego['interval'] == day]\n",
    "    snapshot['origem_id'] = snapshot['src_ip'].map(ip_lookup)\n",
    "    snapshot['destino_id'] = snapshot['destino'].map(ip_lookup)\n",
    "    node_features_day = snapshot.groupby('origem_id').agg({'pkts': ['mean', 'sum']}).reset_index()\n",
    "    node_features_day.columns = ['origem_id', 'media_pkts', 'soma_pkts']\n",
    "    node_features_day.to_csv(f\"../data/features/features_{day}.csv\", index=False)\n",
    "    print(f\"Features do dia {day} salvas em '../data/features/features_{day}.csv'.\")\n",
    "\n",
    "print(\"Extra√ß√£o e salvamento de features conclu√≠dos! üöÄ\")\n",
    "\n",
    "# An√°lise de tr√°fego para NLP\n",
    "print(\"Analisando tr√°fego para NLP...\")\n",
    "raw_df = df_trafego.copy()\n",
    "\n",
    "# Gera√ß√£o do corpus NLP\n",
    "print(\"Gerando corpus NLP...\")\n",
    "tot_intervals = sorted(raw_df['interval'].unique())\n",
    "\n",
    "for day in tqdm(tot_intervals):\n",
    "    # Filtragem do tr√°fego do dia\n",
    "    snapshot = raw_df[raw_df['interval'] == day].sort_values('ts')\n",
    "    # Agrupamento de IPs de origem por porta de destino\n",
    "    corpus = snapshot.groupby('dst_port')['src_ip'].apply(list).to_dict()\n",
    "    # Ordena√ß√£o das portas e alinhamento do corpus\n",
    "    port_order = list(corpus.keys())\n",
    "    corpus_list = [corpus[port] for port in port_order]\n",
    "    # Criar diret√≥rio de sa√≠da\n",
    "    os.makedirs(\"../data/corpus\", exist_ok=True)\n",
    "    # Salvamento do corpus NLP\n",
    "    with open(f'../data/corpus/corpus_{day}.pkl', 'wb') as file:\n",
    "        pickle.dump({'ports': port_order, 'corpus': corpus_list}, file)\n",
    "    print(f\"Corpus do dia {day} salvo com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gera√ß√£o de embeddings (NLP) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abf7e356f9f4050ab178ddaf087f69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando modelo para o dia 1...\n",
      "Embeddings do dia 1 salvos em '../data/nlp_embeddings/embeddings_idarkvec_1.csv'\n",
      "Atualizando modelo para o dia 10...\n",
      "Embeddings do dia 10 salvos em '../data/nlp_embeddings/embeddings_idarkvec_10.csv'\n",
      "Atualizando modelo para o dia 11...\n",
      "Embeddings do dia 11 salvos em '../data/nlp_embeddings/embeddings_idarkvec_11.csv'\n",
      "Atualizando modelo para o dia 12...\n",
      "Embeddings do dia 12 salvos em '../data/nlp_embeddings/embeddings_idarkvec_12.csv'\n",
      "Atualizando modelo para o dia 13...\n",
      "Embeddings do dia 13 salvos em '../data/nlp_embeddings/embeddings_idarkvec_13.csv'\n",
      "Atualizando modelo para o dia 14...\n",
      "Embeddings do dia 14 salvos em '../data/nlp_embeddings/embeddings_idarkvec_14.csv'\n",
      "Atualizando modelo para o dia 15...\n",
      "Embeddings do dia 15 salvos em '../data/nlp_embeddings/embeddings_idarkvec_15.csv'\n",
      "Atualizando modelo para o dia 16...\n",
      "Embeddings do dia 16 salvos em '../data/nlp_embeddings/embeddings_idarkvec_16.csv'\n",
      "Atualizando modelo para o dia 17...\n",
      "Embeddings do dia 17 salvos em '../data/nlp_embeddings/embeddings_idarkvec_17.csv'\n",
      "Atualizando modelo para o dia 18...\n",
      "Embeddings do dia 18 salvos em '../data/nlp_embeddings/embeddings_idarkvec_18.csv'\n",
      "Atualizando modelo para o dia 19...\n",
      "Embeddings do dia 19 salvos em '../data/nlp_embeddings/embeddings_idarkvec_19.csv'\n",
      "Atualizando modelo para o dia 2...\n",
      "Embeddings do dia 2 salvos em '../data/nlp_embeddings/embeddings_idarkvec_2.csv'\n",
      "Atualizando modelo para o dia 20...\n",
      "Embeddings do dia 20 salvos em '../data/nlp_embeddings/embeddings_idarkvec_20.csv'\n",
      "Atualizando modelo para o dia 21...\n",
      "Embeddings do dia 21 salvos em '../data/nlp_embeddings/embeddings_idarkvec_21.csv'\n",
      "Atualizando modelo para o dia 22...\n",
      "Embeddings do dia 22 salvos em '../data/nlp_embeddings/embeddings_idarkvec_22.csv'\n",
      "Atualizando modelo para o dia 23...\n",
      "Embeddings do dia 23 salvos em '../data/nlp_embeddings/embeddings_idarkvec_23.csv'\n",
      "Atualizando modelo para o dia 24...\n",
      "Embeddings do dia 24 salvos em '../data/nlp_embeddings/embeddings_idarkvec_24.csv'\n",
      "Atualizando modelo para o dia 25...\n",
      "Embeddings do dia 25 salvos em '../data/nlp_embeddings/embeddings_idarkvec_25.csv'\n",
      "Atualizando modelo para o dia 26...\n",
      "Embeddings do dia 26 salvos em '../data/nlp_embeddings/embeddings_idarkvec_26.csv'\n",
      "Atualizando modelo para o dia 27...\n",
      "Embeddings do dia 27 salvos em '../data/nlp_embeddings/embeddings_idarkvec_27.csv'\n",
      "Atualizando modelo para o dia 28...\n",
      "Embeddings do dia 28 salvos em '../data/nlp_embeddings/embeddings_idarkvec_28.csv'\n",
      "Atualizando modelo para o dia 29...\n",
      "Embeddings do dia 29 salvos em '../data/nlp_embeddings/embeddings_idarkvec_29.csv'\n",
      "Atualizando modelo para o dia 3...\n",
      "Embeddings do dia 3 salvos em '../data/nlp_embeddings/embeddings_idarkvec_3.csv'\n",
      "Atualizando modelo para o dia 30...\n",
      "Embeddings do dia 30 salvos em '../data/nlp_embeddings/embeddings_idarkvec_30.csv'\n",
      "Atualizando modelo para o dia 31...\n",
      "Embeddings do dia 31 salvos em '../data/nlp_embeddings/embeddings_idarkvec_31.csv'\n",
      "Atualizando modelo para o dia 4...\n",
      "Embeddings do dia 4 salvos em '../data/nlp_embeddings/embeddings_idarkvec_4.csv'\n",
      "Atualizando modelo para o dia 5...\n",
      "Embeddings do dia 5 salvos em '../data/nlp_embeddings/embeddings_idarkvec_5.csv'\n",
      "Atualizando modelo para o dia 6...\n",
      "Embeddings do dia 6 salvos em '../data/nlp_embeddings/embeddings_idarkvec_6.csv'\n",
      "Atualizando modelo para o dia 7...\n",
      "Embeddings do dia 7 salvos em '../data/nlp_embeddings/embeddings_idarkvec_7.csv'\n",
      "Atualizando modelo para o dia 8...\n",
      "Embeddings do dia 8 salvos em '../data/nlp_embeddings/embeddings_idarkvec_8.csv'\n",
      "Atualizando modelo para o dia 9...\n",
      "Embeddings do dia 9 salvos em '../data/nlp_embeddings/embeddings_idarkvec_9.csv'\n"
     ]
    }
   ],
   "source": [
    "from src.models.nlp import iWord2Vec\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Inicializa o modelo\n",
    "word2vec = iWord2Vec(c=5, e=128, epochs=1, seed=15)\n",
    "\n",
    "# Processa os arquivos do corpus\n",
    "for file in tqdm(sorted(glob.glob('../data/corpus/corpus_*.pkl'))):\n",
    "    try:\n",
    "        # Obt√©m o dia a partir do nome do arquivo\n",
    "        day = file.split('/')[-1].replace('corpus_', '').replace('.pkl', '')\n",
    "        \n",
    "        # Valida se o dia √© um n√∫mero entre 1 e 31\n",
    "        try:\n",
    "            day_int = int(day)\n",
    "            if not (1 <= day_int <= 31):\n",
    "                print(f\"Ignorando {file}: dia {day_int} inv√°lido.\")\n",
    "                continue\n",
    "        except ValueError:\n",
    "            print(f\"Ignorando {file}: nome do arquivo n√£o cont√©m um dia v√°lido.\")\n",
    "            continue\n",
    "        \n",
    "        # Carrega o corpus\n",
    "        with open(file, 'rb') as f:\n",
    "            corpus_data = pickle.load(f)\n",
    "        \n",
    "        # Verifica a estrutura esperada\n",
    "        if 'ports' not in corpus_data or 'corpus' not in corpus_data:\n",
    "            print(f\"Estrutura inv√°lida em {file}. Ignorando.\")\n",
    "            continue\n",
    "        \n",
    "        port_order = corpus_data['ports']\n",
    "        corpus = corpus_data['corpus']\n",
    "        \n",
    "        # Verifica se o corpus est√° vazio\n",
    "        if not corpus:\n",
    "            print(f\"Corpus vazio em {file}. Ignorando.\")\n",
    "            continue\n",
    "        \n",
    "        # Treina ou atualiza o modelo conforme o dia\n",
    "        if day_int == 1:\n",
    "            print(f\"Treinando modelo para o dia {day_int}...\")\n",
    "            word2vec.train(corpus)\n",
    "        else:\n",
    "            print(f\"Atualizando modelo para o dia {day_int}...\")\n",
    "            word2vec.update(corpus)\n",
    "        \n",
    "        # Obt√©m os embeddings\n",
    "        embeddings = word2vec.get_embeddings()\n",
    "        if isinstance(embeddings, pd.DataFrame):\n",
    "            embeddings.index.name = \"src_ip\"\n",
    "            embeddings = embeddings.reset_index()\n",
    "        else:\n",
    "            embeddings = pd.DataFrame(embeddings)\n",
    "            embeddings.insert(0, \"src_ip\", port_order)\n",
    "        \n",
    "        # Salva os embeddings em CSV\n",
    "        os.makedirs(\"../data/nlp_embeddings\", exist_ok=True)\n",
    "        embeddings.to_csv(f'../data/nlp_embeddings/embeddings_idarkvec_{day}.csv', index=False)\n",
    "        \n",
    "        print(f\"Embeddings do dia {day} salvos em '../data/nlp_embeddings/embeddings_idarkvec_{day}.csv'\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifica√ß√£o (NLP) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificando conte√∫do do arquivo:\n",
      "         src_ip         0         1         2         3         4         5  \\\n",
      "0  192.168.1.26  0.006738  0.003012  0.003212  0.004935 -0.004036 -0.002431   \n",
      "1  192.168.1.33  0.002965 -0.003778  0.000793 -0.001849 -0.000688 -0.006697   \n",
      "2  192.168.1.40 -0.001823  0.007714  0.007047 -0.003224  0.007148  0.004393   \n",
      "3  192.168.1.38 -0.005350  0.004542 -0.001045  0.005886 -0.000692 -0.002025   \n",
      "4   192.168.1.9 -0.002121 -0.001018 -0.006599 -0.000633 -0.002287  0.005048   \n",
      "\n",
      "          6         7         8  ...       118       119       120       121  \\\n",
      "0 -0.004646 -0.007112 -0.000888  ... -0.006956  0.003519  0.003514  0.004902   \n",
      "1 -0.006737  0.003017 -0.004178  ... -0.004152  0.002504 -0.000645  0.005921   \n",
      "2 -0.002487 -0.000087  0.002947  ...  0.004844  0.000537  0.001595  0.000475   \n",
      "3  0.004510  0.002709 -0.003699  ... -0.000646 -0.004903 -0.001364 -0.001819   \n",
      "4  0.000739 -0.003050 -0.006083  ... -0.003366 -0.004525 -0.001277 -0.005287   \n",
      "\n",
      "        122       123       124       125       126       127  \n",
      "0 -0.005652  0.007779  0.000233 -0.000664 -0.002660 -0.004676  \n",
      "1 -0.004516  0.002195  0.006136  0.000307  0.001234 -0.002527  \n",
      "2 -0.002731 -0.000327 -0.002062  0.000400 -0.000519  0.004494  \n",
      "3  0.002665  0.006902  0.001184 -0.002025  0.001991  0.004633  \n",
      "4 -0.007148  0.006845  0.005068 -0.004754 -0.004283 -0.006139  \n",
      "\n",
      "[5 rows x 129 columns]\n",
      "\n",
      "Verificando estrutura dos embeddings...\n",
      "         src_ip         0         1         2         3         4         5  \\\n",
      "0  192.168.1.26  0.006738  0.003012  0.003212  0.004935 -0.004036 -0.002431   \n",
      "1  192.168.1.33  0.002965 -0.003778  0.000793 -0.001849 -0.000688 -0.006697   \n",
      "2  192.168.1.40 -0.001823  0.007714  0.007047 -0.003224  0.007148  0.004393   \n",
      "3  192.168.1.38 -0.005350  0.004542 -0.001045  0.005886 -0.000692 -0.002025   \n",
      "4   192.168.1.9 -0.002121 -0.001018 -0.006599 -0.000633 -0.002287  0.005048   \n",
      "\n",
      "          6         7         8  ...       118       119       120       121  \\\n",
      "0 -0.004646 -0.007112 -0.000888  ... -0.006956  0.003519  0.003514  0.004902   \n",
      "1 -0.006737  0.003017 -0.004178  ... -0.004152  0.002504 -0.000645  0.005921   \n",
      "2 -0.002487 -0.000087  0.002947  ...  0.004844  0.000537  0.001595  0.000475   \n",
      "3  0.004510  0.002709 -0.003699  ... -0.000646 -0.004903 -0.001364 -0.001819   \n",
      "4  0.000739 -0.003050 -0.006083  ... -0.003366 -0.004525 -0.001277 -0.005287   \n",
      "\n",
      "        122       123       124       125       126       127  \n",
      "0 -0.005652  0.007779  0.000233 -0.000664 -0.002660 -0.004676  \n",
      "1 -0.004516  0.002195  0.006136  0.000307  0.001234 -0.002527  \n",
      "2 -0.002731 -0.000327 -0.002062  0.000400 -0.000519  0.004494  \n",
      "3  0.002665  0.006902  0.001184 -0.002025  0.001991  0.004633  \n",
      "4 -0.007148  0.006845  0.005068 -0.004754 -0.004283 -0.006139  \n",
      "\n",
      "[5 rows x 129 columns]\n",
      "\n",
      "Distribui√ß√£o de r√≥tulos antes da filtragem:\n",
      "label\n",
      "unknown            7\n",
      "mirai              2\n",
      "benign             1\n",
      "unk_bruteforcer    1\n",
      "securitytrails     1\n",
      "internetcensus     1\n",
      "censys             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribui√ß√£o de r√≥tulos ap√≥s a filtragem:\n",
      "label\n",
      "mirai              2\n",
      "benign             1\n",
      "unk_bruteforcer    1\n",
      "securitytrails     1\n",
      "internetcensus     1\n",
      "censys             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Relat√≥rio de Classifica√ß√£o:\n",
      "                 precision    recall  f1-score   support\n",
      "benign            0.500000  1.000000  0.666667  1.000000\n",
      "censys            0.000000  0.000000  0.000000  1.000000\n",
      "internetcensus    0.000000  0.000000  0.000000  1.000000\n",
      "mirai             0.000000  0.000000  0.000000  2.000000\n",
      "securitytrails    0.000000  0.000000  0.000000  1.000000\n",
      "unk_bruteforcer   0.000000  0.000000  0.000000  1.000000\n",
      "accuracy          0.142857  0.142857  0.142857  0.142857\n",
      "macro avg         0.083333  0.166667  0.111111  7.000000\n",
      "weighted avg      0.071429  0.142857  0.095238  7.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Carrega o arquivo de ground truth contendo os r√≥tulos reais\n",
    "gt_file = 'ground_truth_simulado.csv'\n",
    "if not os.path.exists(gt_file):\n",
    "    raise FileNotFoundError(f\"Arquivo de ground truth {gt_file} n√£o encontrado.\")\n",
    "\n",
    "gt = pd.read_csv(gt_file)\n",
    "\n",
    "gt['src_ip'] = gt['src_ip'].astype(str)  # Garante que os endere√ßos IP sejam tratados como strings\n",
    "\n",
    "if 'src_ip' not in gt.columns or 'label' not in gt.columns:\n",
    "    raise ValueError(\"O ground truth deve conter as colunas 'src_ip' e 'label'.\")\n",
    "\n",
    "# Implementa√ß√£o de um classificador k-NN personalizado com vizinhos adaptativos\n",
    "class KnnClassifier:\n",
    "    def __init__(self, n_neighbors=3, metric='cosine'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "        self.model = None  # O modelo ser√° inicializado na chamada do m√©todo `fit`\n",
    "\n",
    "    def fit(self, X_train, y_train, scale_data=False):\n",
    "        num_samples = len(y_train)\n",
    "\n",
    "        k = min(self.n_neighbors, num_samples)  # Ajusta `k` para n√£o ser maior que o n√∫mero de amostras dispon√≠veis\n",
    "        if k < 1:\n",
    "            raise ValueError(\"N√∫mero insuficiente de amostras para classifica√ß√£o k-NN.\")\n",
    "\n",
    "        self.model = KNeighborsClassifier(n_neighbors=k, metric=self.metric)\n",
    "\n",
    "        if scale_data:\n",
    "            self.scaler = StandardScaler()\n",
    "            X_train = self.scaler.fit_transform(X_train)  # Normaliza os dados antes do treinamento\n",
    "\n",
    "        self.model.fit(X_train, y_train)  # Treina o modelo com os dados fornecidos\n",
    "\n",
    "    def predict(self, X_test, scale_data=False):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"O modelo ainda n√£o foi treinado.\")\n",
    "\n",
    "        if scale_data:\n",
    "            X_test = self.scaler.transform(X_test)  # Normaliza os dados antes da predi√ß√£o\n",
    "\n",
    "        return self.model.predict(X_test)  # Retorna as previs√µes do modelo\n",
    "\n",
    "# Pipeline de classifica√ß√£o\n",
    "def classification_pipeline(embeddings, gt):\n",
    "    print(\"\\nVerificando estrutura dos embeddings...\")\n",
    "    print(embeddings.head())  # Exibe amostras dos embeddings para verifica√ß√£o\n",
    "\n",
    "    if 'src_ip' not in embeddings.columns:\n",
    "        raise ValueError(\"Arquivo de embeddings n√£o cont√©m 'src_ip'. Verifique o formato.\")\n",
    "\n",
    "    embeddings['src_ip'] = embeddings['src_ip'].astype(str)  # Garante que os endere√ßos IP sejam strings\n",
    "\n",
    "    # Mescla os embeddings com o ground truth com base no campo 'src_ip'\n",
    "    embeddings = embeddings.merge(gt, on='src_ip', how='left').fillna('unknown')\n",
    "\n",
    "    print(\"\\nDistribui√ß√£o de r√≥tulos antes da filtragem:\")\n",
    "    print(embeddings['label'].value_counts())  # Mostra a distribui√ß√£o dos r√≥tulos antes da filtragem\n",
    "\n",
    "    valid_labels = embeddings['label'].value_counts().index\n",
    "    embeddings.loc[~embeddings['label'].isin(valid_labels), 'label'] = 'unknown'\n",
    "\n",
    "    # Remove registros com r√≥tulo 'unknown' apenas se houver outras classes v√°lidas\n",
    "    if (embeddings['label'] == 'unknown').sum() != len(embeddings):\n",
    "        embeddings = embeddings[embeddings['label'] != 'unknown']\n",
    "\n",
    "    print(\"\\nDistribui√ß√£o de r√≥tulos ap√≥s a filtragem:\")\n",
    "    print(embeddings['label'].value_counts())  # Exibe a distribui√ß√£o dos r√≥tulos ap√≥s a filtragem\n",
    "\n",
    "    # Verifica se h√° classes suficientes para treinar o modelo\n",
    "    if len(np.unique(embeddings['label'].values)) < 2:\n",
    "        raise ValueError(\"\\nErro: quantidade insuficiente de classes v√°lidas para classifica√ß√£o. Verifique a distribui√ß√£o de r√≥tulos.\")\n",
    "\n",
    "    X_train = embeddings.drop(columns=['label', 'src_ip'], errors='ignore').values  # Remove colunas n√£o num√©ricas\n",
    "    y_train = np.ravel(embeddings[['label']].values)  # Obt√©m os r√≥tulos para treinamento\n",
    "\n",
    "    knn = KnnClassifier(n_neighbors=3, metric='cosine')\n",
    "    knn.fit(X_train, y_train, scale_data=True)  # Treina o classificador\n",
    "\n",
    "    valid_indices = np.arange(len(y_train)).astype(int).flatten()\n",
    "\n",
    "    y_true = y_train[valid_indices]  # Obt√©m os r√≥tulos verdadeiros\n",
    "    y_pred = knn.predict(X_train[valid_indices], scale_data=True)  # Obt√©m as previs√µes do modelo\n",
    "\n",
    "    crep = classification_report(y_true, y_pred, labels=np.unique(y_true), output_dict=True)  # Gera o relat√≥rio de classifica√ß√£o\n",
    "    return crep\n",
    "\n",
    "# Carrega o arquivo de embeddings e executa a classifica√ß√£o\n",
    "embedding_file = '../data/nlp_embeddings/embeddings_idarkvec_2.csv'\n",
    "if not os.path.exists(embedding_file):\n",
    "    raise FileNotFoundError(f\"Arquivo de embeddings {embedding_file} n√£o encontrado.\")\n",
    "\n",
    "embeddings = pd.read_csv(embedding_file)\n",
    "\n",
    "print(\"\\nVerificando conte√∫do do arquivo:\")\n",
    "print(embeddings.head())  # Exibe amostras do arquivo de embeddings para depura√ß√£o\n",
    "\n",
    "if 'src_ip' not in embeddings.columns:\n",
    "    raise KeyError(\"A coluna 'src_ip' est√° ausente no arquivo de embeddings. Verifique o formato dos dados.\")\n",
    "\n",
    "embeddings['src_ip'] = embeddings['src_ip'].astype(str)  # Converte os IPs para string para garantir consist√™ncia\n",
    "\n",
    "classification_report = classification_pipeline(embeddings, gt)  # Executa o pipeline de classifica√ß√£o\n",
    "\n",
    "print(\"\\nRelat√≥rio de Classifica√ß√£o:\")\n",
    "print(pd.DataFrame(classification_report).transpose())  # Exibe o relat√≥rio de classifica√ß√£o final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gera√ß√£o de embeddings (GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando matriz_adjacencia_simulada.txt...\n",
      "matriz_adjacencia_simulada.txt - Dados convertidos para IDs num√©ricos:\n",
      "   src  dst  weight\n",
      "0   12   26       1\n",
      "1   12   32       1\n",
      "2   12   44       1\n",
      "3   12   45       1\n",
      "4    8    2       1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d592a6e5e649519d9bfd24ccd3a995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Treinando GCN nos snapshots:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stop condition met\n",
      "Embeddings do Dia 1 salvos em ../data/gnn_embeddings/embeddings_gcn_features_1.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 2 salvos em ../data/gnn_embeddings/embeddings_gcn_features_2.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 3 salvos em ../data/gnn_embeddings/embeddings_gcn_features_3.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 4 salvos em ../data/gnn_embeddings/embeddings_gcn_features_4.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 5 salvos em ../data/gnn_embeddings/embeddings_gcn_features_5.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 6 salvos em ../data/gnn_embeddings/embeddings_gcn_features_6.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 7 salvos em ../data/gnn_embeddings/embeddings_gcn_features_7.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 8 salvos em ../data/gnn_embeddings/embeddings_gcn_features_8.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 9 salvos em ../data/gnn_embeddings/embeddings_gcn_features_9.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 10 salvos em ../data/gnn_embeddings/embeddings_gcn_features_10.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 11 salvos em ../data/gnn_embeddings/embeddings_gcn_features_11.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 12 salvos em ../data/gnn_embeddings/embeddings_gcn_features_12.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 13 salvos em ../data/gnn_embeddings/embeddings_gcn_features_13.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 14 salvos em ../data/gnn_embeddings/embeddings_gcn_features_14.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 15 salvos em ../data/gnn_embeddings/embeddings_gcn_features_15.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 16 salvos em ../data/gnn_embeddings/embeddings_gcn_features_16.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 17 salvos em ../data/gnn_embeddings/embeddings_gcn_features_17.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 18 salvos em ../data/gnn_embeddings/embeddings_gcn_features_18.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 19 salvos em ../data/gnn_embeddings/embeddings_gcn_features_19.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 20 salvos em ../data/gnn_embeddings/embeddings_gcn_features_20.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 21 salvos em ../data/gnn_embeddings/embeddings_gcn_features_21.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 22 salvos em ../data/gnn_embeddings/embeddings_gcn_features_22.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 23 salvos em ../data/gnn_embeddings/embeddings_gcn_features_23.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 24 salvos em ../data/gnn_embeddings/embeddings_gcn_features_24.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 25 salvos em ../data/gnn_embeddings/embeddings_gcn_features_25.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 26 salvos em ../data/gnn_embeddings/embeddings_gcn_features_26.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 27 salvos em ../data/gnn_embeddings/embeddings_gcn_features_27.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 28 salvos em ../data/gnn_embeddings/embeddings_gcn_features_28.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 29 salvos em ../data/gnn_embeddings/embeddings_gcn_features_29.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 30 salvos em ../data/gnn_embeddings/embeddings_gcn_features_30.csv\n",
      "Early stop condition met\n",
      "Embeddings do Dia 31 salvos em ../data/gnn_embeddings/embeddings_gcn_features_31.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from collections import deque  # Para armazenar as matrizes de adjac√™ncia passadas\n",
    "from src.preprocessing.gnn import generate_adjacency_matrices\n",
    "from src.models.gnn import GCN\n",
    "from glob import glob\n",
    "\n",
    "epochs = 50  # N√∫mero de √©pocas de treinamento\n",
    "HISTORY_DAYS = 5  # N√∫mero de snapshots passados a serem usados\n",
    "\n",
    "# Caminhos para os diret√≥rios de entrada e sa√≠da\n",
    "DATASET_DIR = '../data'\n",
    "GRAPH_DIR = f'{DATASET_DIR}/graph'\n",
    "OUTPUT_DIR = f'{DATASET_DIR}/gnn_embeddings'\n",
    "\n",
    "# Criar o diret√≥rio de sa√≠da caso ele n√£o exista\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Carregar o dicion√°rio de IPs\n",
    "ip_lookup_path = f'{GRAPH_DIR}/ip_lookup.json'\n",
    "if not os.path.exists(ip_lookup_path):\n",
    "    raise FileNotFoundError(f\"Arquivo de mapeamento de IPs n√£o encontrado: {ip_lookup_path}\")\n",
    "\n",
    "with open(ip_lookup_path, 'r') as file:\n",
    "    ip_lookup = json.load(file)\n",
    "    reverse_lookup = {v: k for k, v in ip_lookup.items()}\n",
    "    ip_nodes = len(reverse_lookup)\n",
    "\n",
    "# Carregar a √∫nica matriz de adjac√™ncia dispon√≠vel\n",
    "graph_file = sorted(glob('*.txt'))\n",
    "\n",
    "\n",
    "if not graph_file:\n",
    "    raise FileNotFoundError(f\"Nenhuma matriz de adjac√™ncia encontrada em {GRAPH_DIR}\")\n",
    "\n",
    "# Gerar a matriz de adjac√™ncia √∫nica\n",
    "X_train = generate_adjacency_matrices(graph_file, ip_lookup_path, weighted=True)[0]\n",
    "n_nodes = X_train.shape[0]  # N√∫mero total de n√≥s\n",
    "\n",
    "# Converter a matriz de adjac√™ncia para o formato esparso\n",
    "X_train = X_train.to_sparse()\n",
    "\n",
    "# Fila para armazenar os √∫ltimos 5 snapshots\n",
    "history_queue = deque(maxlen=HISTORY_DAYS)\n",
    "\n",
    "# Inicializar o modelo GCN uma √∫nica vez (mant√©m a mem√≥ria do treinamento anterior)\n",
    "gcn = GCN(\n",
    "    n_nodes=n_nodes, \n",
    "    ns=1, \n",
    "    gcn_layers=2, \n",
    "    input_size=n_nodes, \n",
    "    gcn_units=1024, \n",
    "    gcn_output_size=512,\n",
    "    embedding_size=128, \n",
    "    predictor_units=64, \n",
    "    dropout=0.0, \n",
    "    lr=1e-3, \n",
    "    cuda=torch.cuda.is_available(), \n",
    "    epochs=epochs,\n",
    "    early_stop=10  # Permite que o modelo pare se n√£o houver melhora por 10 √©pocas consecutivas\n",
    ")\n",
    "\n",
    "# Loop para processar os 31 dias\n",
    "for i in tqdm(range(31), desc=\"Treinando GCN nos snapshots\"):\n",
    "    \n",
    "    # Caso tenha hist√≥rico suficiente, calcular a m√©dia das √∫ltimas 5 matrizes de adjac√™ncia\n",
    "    if len(history_queue) >= HISTORY_DAYS:\n",
    "        X_dense_list = [x.to_dense() for x in history_queue]  # Converter todas as matrizes esparsas para densas\n",
    "        X_input = torch.stack(X_dense_list).mean(dim=0).to_sparse()  # Calcular a m√©dia e converter de volta para esparsa\n",
    "    else:\n",
    "        X_input = X_train  # Usar a matriz de adjac√™ncia original caso o hist√≥rico n√£o esteja completo\n",
    "\n",
    "    # Garantir que a matriz de entrada esteja no formato esparso adequado\n",
    "    X_input = X_input.coalesce()\n",
    "\n",
    "    # Treinar o modelo usando a mesma matriz de adjac√™ncia, mas atualizando os embeddings\n",
    "    gcn.fit(X_input, day=i)\n",
    "\n",
    "    # Obter os embeddings considerando o hist√≥rico aprendido\n",
    "    embeddings = gcn.get_embeddings(X_input)[:ip_nodes]\n",
    "\n",
    "    # Ajustar o √≠ndice com os n√≥s de IPs\n",
    "    new_index = [reverse_lookup[x] for x in range(ip_nodes)]\n",
    "    embeddings = pd.DataFrame(embeddings, index=new_index)\n",
    "\n",
    "    # Formatar o nome do dia para salvar corretamente\n",
    "    day = f\"{i+1}\"  \n",
    "\n",
    "    # Caminho para salvar os embeddings\n",
    "    ename= f'{OUTPUT_DIR}/embeddings_gcn_features_{day}.csv'\n",
    "    embeddings.to_csv(ename)\n",
    "\n",
    "    print(f\"Embeddings do Dia {day} salvos em {ename}\")\n",
    "\n",
    "    # Armazenar o snapshot atual na fila de hist√≥rico para uso futuro\n",
    "    history_queue.append(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando matriz_adjacencia_simulada.txt...\n",
      "matriz_adjacencia_simulada.txt - Dados convertidos para IDs num√©ricos:\n",
      "   src  dst  weight\n",
      "0   12   26       1\n",
      "1   12   32       1\n",
      "2   12   44       1\n",
      "3   12   45       1\n",
      "4    8    2       1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8174e64d9e49d9b6faad35e2fa0e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando Snapshots:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando Dia 1:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 1 salvos em ../data/gnn_embeddings/embeddings_gcn_features_1.csv\n",
      "Processando Dia 2:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 2 salvos em ../data/gnn_embeddings/embeddings_gcn_features_2.csv\n",
      "Processando Dia 3:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 3 salvos em ../data/gnn_embeddings/embeddings_gcn_features_3.csv\n",
      "Processando Dia 4:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 4 salvos em ../data/gnn_embeddings/embeddings_gcn_features_4.csv\n",
      "Processando Dia 5:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 5 salvos em ../data/gnn_embeddings/embeddings_gcn_features_5.csv\n",
      "Processando Dia 6:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 6 salvos em ../data/gnn_embeddings/embeddings_gcn_features_6.csv\n",
      "Processando Dia 7:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 7 salvos em ../data/gnn_embeddings/embeddings_gcn_features_7.csv\n",
      "Processando Dia 8:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 8 salvos em ../data/gnn_embeddings/embeddings_gcn_features_8.csv\n",
      "Processando Dia 9:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 9 salvos em ../data/gnn_embeddings/embeddings_gcn_features_9.csv\n",
      "Processando Dia 10:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 10 salvos em ../data/gnn_embeddings/embeddings_gcn_features_10.csv\n",
      "Processando Dia 11:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 11 salvos em ../data/gnn_embeddings/embeddings_gcn_features_11.csv\n",
      "Processando Dia 12:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 12 salvos em ../data/gnn_embeddings/embeddings_gcn_features_12.csv\n",
      "Processando Dia 13:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 13 salvos em ../data/gnn_embeddings/embeddings_gcn_features_13.csv\n",
      "Processando Dia 14:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 14 salvos em ../data/gnn_embeddings/embeddings_gcn_features_14.csv\n",
      "Processando Dia 15:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 15 salvos em ../data/gnn_embeddings/embeddings_gcn_features_15.csv\n",
      "Processando Dia 16:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 16 salvos em ../data/gnn_embeddings/embeddings_gcn_features_16.csv\n",
      "Processando Dia 17:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 17 salvos em ../data/gnn_embeddings/embeddings_gcn_features_17.csv\n",
      "Processando Dia 18:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 18 salvos em ../data/gnn_embeddings/embeddings_gcn_features_18.csv\n",
      "Processando Dia 19:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 19 salvos em ../data/gnn_embeddings/embeddings_gcn_features_19.csv\n",
      "Processando Dia 20:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 20 salvos em ../data/gnn_embeddings/embeddings_gcn_features_20.csv\n",
      "Processando Dia 21:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 21 salvos em ../data/gnn_embeddings/embeddings_gcn_features_21.csv\n",
      "Processando Dia 22:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 22 salvos em ../data/gnn_embeddings/embeddings_gcn_features_22.csv\n",
      "Processando Dia 23:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 23 salvos em ../data/gnn_embeddings/embeddings_gcn_features_23.csv\n",
      "Processando Dia 24:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 24 salvos em ../data/gnn_embeddings/embeddings_gcn_features_24.csv\n",
      "Processando Dia 25:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 25 salvos em ../data/gnn_embeddings/embeddings_gcn_features_25.csv\n",
      "Processando Dia 26:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 26 salvos em ../data/gnn_embeddings/embeddings_gcn_features_26.csv\n",
      "Processando Dia 27:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 27 salvos em ../data/gnn_embeddings/embeddings_gcn_features_27.csv\n",
      "Processando Dia 28:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 28 salvos em ../data/gnn_embeddings/embeddings_gcn_features_28.csv\n",
      "Processando Dia 29:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 29 salvos em ../data/gnn_embeddings/embeddings_gcn_features_29.csv\n",
      "Processando Dia 30:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 30 salvos em ../data/gnn_embeddings/embeddings_gcn_features_30.csv\n",
      "Processando Dia 31:\n",
      "X_train shape: torch.Size([50, 50])\n",
      "features_train_day shape: torch.Size([50, 2])\n",
      "Early stop condition met\n",
      "Embeddings do dia 31 salvos em ../data/gnn_embeddings/embeddings_gcn_features_31.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from src.preprocessing.gnn import generate_adjacency_matrices\n",
    "from src.models.gnn import GCN\n",
    "\n",
    "# N√∫mero de √©pocas de treinamento\n",
    "EPOCHS = 50\n",
    "# Caminho para os dados\n",
    "DATASET_DIR = '../data'  # Ajuste conforme necess√°rio\n",
    "FEATURES_DIR = f'{DATASET_DIR}/features'\n",
    "GRAPH_DIR = f'{DATASET_DIR}/graph'\n",
    "OUTPUT_DIR = f'{DATASET_DIR}/gnn_embeddings'\n",
    "\n",
    "# Criar diret√≥rio de sa√≠da se n√£o existir\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Carregar dicion√°rio de IPs\n",
    "ip_lookup_path = f'{GRAPH_DIR}/ip_lookup.json'\n",
    "if not os.path.exists(ip_lookup_path):\n",
    "    raise FileNotFoundError(f\"Arquivo de lookup de IPs n√£o encontrado: {ip_lookup_path}\")\n",
    "\n",
    "with open(ip_lookup_path, 'r') as file:\n",
    "    ip_lookup = json.load(file)\n",
    "    reverse_lookup = {v: k for k, v in ip_lookup.items()}\n",
    "    ip_nodes = len(reverse_lookup)\n",
    "\n",
    "# Carregar matriz de adjac√™ncia\n",
    "graph_files = sorted(glob('*.txt'))\n",
    "\n",
    "if not graph_files:\n",
    "    raise FileNotFoundError(f\"Nenhuma matriz de adjac√™ncia encontrada em {GRAPH_DIR}\")\n",
    "\n",
    "# Gerar matriz de adjac√™ncia √∫nica (assumindo um √∫nico arquivo)\n",
    "X_train = generate_adjacency_matrices(graph_files, ip_lookup_path, weighted=True)[0]\n",
    "n_nodes = X_train.shape[0]  # N√∫mero total de n√≥s\n",
    "\n",
    "# Listar arquivos de features (um para cada dia)\n",
    "feature_files = sorted(glob(f'{FEATURES_DIR}/*.csv'))\n",
    "\n",
    "# Processar cada dia **individualmente**, mantendo a mesma matriz de adjac√™ncia\n",
    "for i, feature_file in tqdm(enumerate(feature_files), total=len(feature_files), desc=\"Processando Snapshots\"):\n",
    "    \n",
    "    # Carregar features do dia i\n",
    "    feat = pd.read_csv(feature_file, index_col=[0]).sort_index()\n",
    "    features_train_day = torch.tensor(feat.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    # Ajustar o n√∫mero de n√≥s para alinhar features com a matriz de adjac√™ncia\n",
    "    n_features = features_train_day.shape[1]\n",
    "\n",
    "    # Garantir que features_train_day tenha o mesmo n√∫mero de n√≥s que X_train\n",
    "    fixed_feat = torch.zeros((n_nodes, n_features))  # Criar tensor zerado\n",
    "    num_feat_nodes = min(features_train_day.shape[0], n_nodes)\n",
    "    fixed_feat[:num_feat_nodes, :] = features_train_day[:num_feat_nodes, :]  # Copiar dados existentes\n",
    "    features_train_day = fixed_feat\n",
    "\n",
    "    print(f\"Processando Dia {i+1}:\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")  # Deve ser (n_nodes, n_nodes)\n",
    "    print(f\"features_train_day shape: {features_train_day.shape}\")  # Deve ser (n_nodes, n_features)\n",
    "\n",
    "    # Inicializar o modelo GCN **apenas para esse dia**\n",
    "    gcn = GCN(\n",
    "        n_nodes=n_nodes,\n",
    "        ns=1,\n",
    "        gcn_layers=2,\n",
    "        input_size=n_features,\n",
    "        gcn_units=1024,\n",
    "        gcn_output_size=512,\n",
    "        embedding_size=128,\n",
    "        predictor_units=64,\n",
    "        dropout=0.0,\n",
    "        lr=1e-3,\n",
    "        cuda=torch.cuda.is_available(),\n",
    "        epochs=EPOCHS,\n",
    "        early_stop=3\n",
    "    )\n",
    "\n",
    "    # Treinar o modelo **apenas com os dados do dia i**\n",
    "    gcn.fit(X_train, features=features_train_day, day=i)\n",
    "\n",
    "    # Obter os embeddings **apenas desse dia**\n",
    "    embeddings = gcn.get_embeddings(X_train, features_train_day)[:ip_nodes]\n",
    "\n",
    "    # Ajustar √≠ndice com os n√≥s de IPs\n",
    "    new_index = [reverse_lookup[x] for x in range(ip_nodes)]\n",
    "    embeddings = pd.DataFrame(embeddings, index=new_index)\n",
    "    embeddings.index.name = \"src_ip\" \n",
    "\n",
    "    # Formatar o nome do dia para salvar corretamente\n",
    "    day = f\"{i+1}\"  \n",
    "\n",
    "    # Caminho para salvar os embeddings\n",
    "    embedding_path = f'{OUTPUT_DIR}/embeddings_gcn_features_{day}.csv'\n",
    "    embeddings.to_csv(embedding_path)\n",
    "\n",
    "    print(f\"Embeddings do dia {day} salvos em {embedding_path}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifica√ß√£o (GCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processando embeddings do dia 1...\n",
      "\n",
      "Verificando estrutura dos embeddings...\n",
      "         src_ip         0         1         2         3         4         5  \\\n",
      "0   192.168.1.3 -0.446166 -0.280962  0.004478 -0.299272  0.004644  0.002221   \n",
      "1  192.168.1.33  5.811852  4.657792  0.004478  5.178214  0.004644  0.002221   \n",
      "2   192.168.1.6 -0.446166  2.422536  0.004478  1.601347  0.004644  0.002221   \n",
      "3  192.168.1.50 -0.098742  0.150052  0.004478  0.179612  0.004644  0.002221   \n",
      "4  192.168.1.10  0.019109 -0.751369  0.004478 -0.640602  0.004644  0.002221   \n",
      "\n",
      "         6         7         8  ...       118       119       120       121  \\\n",
      "0 -0.12294 -0.134398 -0.141837  ... -0.001993 -0.316058 -0.296959  0.005201   \n",
      "1 -0.12294 -0.134398 -0.141837  ... -0.001993  5.577607  4.997903  0.005201   \n",
      "2 -0.12294 -0.134398 -0.141837  ... -0.001993  0.493747  1.913123  0.005201   \n",
      "3 -0.12294 -0.134398 -0.141837  ... -0.001993  0.200628  0.165536  0.005201   \n",
      "4 -0.12294 -0.134398 -0.141837  ... -0.001993 -0.475798 -0.690325  0.005201   \n",
      "\n",
      "        122       123       124       125       126       127  \n",
      "0 -0.001245  0.002469 -0.289185  0.000927 -0.301893 -0.133177  \n",
      "1 -0.001245  0.002469  4.715669  0.000927  5.158625 -0.133177  \n",
      "2 -0.001245  0.002469  2.324816  0.000927  1.622936 -0.133177  \n",
      "3 -0.001245  0.002469  0.145607  0.000927  0.172260 -0.133177  \n",
      "4 -0.001245  0.002469 -0.750402  0.000927 -0.647634 -0.133177  \n",
      "\n",
      "[5 rows x 129 columns]\n",
      "\n",
      "Distribui√ß√£o de r√≥tulos antes da filtragem:\n",
      "label\n",
      "unknown            30\n",
      "internetcensus      4\n",
      "mirai               3\n",
      "rapid7              2\n",
      "intrinsec           2\n",
      "benign              2\n",
      "unk_exploiter       2\n",
      "driftnet            1\n",
      "securitytrails      1\n",
      "censys              1\n",
      "unk_bruteforcer     1\n",
      "unk_spammer         1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribui√ß√£o de r√≥tulos ap√≥s a filtragem:\n",
      "label\n",
      "internetcensus     4\n",
      "mirai              3\n",
      "rapid7             2\n",
      "intrinsec          2\n",
      "benign             2\n",
      "unk_exploiter      2\n",
      "driftnet           1\n",
      "securitytrails     1\n",
      "censys             1\n",
      "unk_bruteforcer    1\n",
      "unk_spammer        1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '../data/results'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 110\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# Salvar os resultados do dia\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(crep)\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[0;32m--> 110\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/results/classification_gcn_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mday\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     final_reps\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Concatenar os resultados de todos os dias\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '../data/results'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Carrega o arquivo de ground truth contendo os r√≥tulos reais\n",
    "gt_file = 'ground_truth_simulado.csv'\n",
    "if not os.path.exists(gt_file):\n",
    "    raise FileNotFoundError(f\"Arquivo de ground truth {gt_file} n√£o encontrado.\")\n",
    "\n",
    "gt = pd.read_csv(gt_file)\n",
    "gt['src_ip'] = gt['src_ip'].astype(str)  # Garante que os endere√ßos IP sejam strings\n",
    "\n",
    "if 'src_ip' not in gt.columns or 'label' not in gt.columns:\n",
    "    raise ValueError(\"O ground truth deve conter as colunas 'src_ip' e 'label'.\")\n",
    "\n",
    "# Implementa√ß√£o de um classificador k-NN personalizado com Leave-One-Out\n",
    "class KnnClassifier:\n",
    "    def __init__(self, n_neighbors=3, metric='cosine'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "        self.scaler = StandardScaler()  # Normalizador para os dados\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.model = KNeighborsClassifier(n_neighbors=min(self.n_neighbors, len(y_train)), metric=self.metric)\n",
    "        X_train = self.scaler.fit_transform(X_train)  # Normaliza os dados\n",
    "        self.model.fit(X_train, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = self.scaler.transform(X_test)  # Normaliza os dados antes da predi√ß√£o\n",
    "        return self.model.predict(X_test)\n",
    "\n",
    "# Pipeline de classifica√ß√£o usando Leave-One-Out\n",
    "def classification_pipeline(embeddings, gt):\n",
    "    print(\"\\nVerificando estrutura dos embeddings...\")\n",
    "    print(embeddings.head())  # Exibe amostras dos embeddings para verifica√ß√£o\n",
    "\n",
    "    if 'src_ip' not in embeddings.columns:\n",
    "        raise ValueError(\"Arquivo de embeddings n√£o cont√©m 'src_ip'. Verifique o formato.\")\n",
    "\n",
    "    embeddings['src_ip'] = embeddings['src_ip'].astype(str)  # Garante que os endere√ßos IP sejam strings\n",
    "\n",
    "    # Mescla os embeddings com o ground truth com base no campo 'src_ip'\n",
    "    embeddings = embeddings.merge(gt, on='src_ip', how='left').fillna('unknown')\n",
    "\n",
    "    print(\"\\nDistribui√ß√£o de r√≥tulos antes da filtragem:\")\n",
    "    print(embeddings['label'].value_counts())  # Mostra a distribui√ß√£o dos r√≥tulos antes da filtragem\n",
    "\n",
    "    valid_labels = embeddings['label'].value_counts().index\n",
    "    embeddings.loc[~embeddings['label'].isin(valid_labels), 'label'] = 'unknown'\n",
    "\n",
    "    # Remove registros com r√≥tulo 'unknown' apenas se houver outras classes v√°lidas\n",
    "    if (embeddings['label'] == 'unknown').sum() != len(embeddings):\n",
    "        embeddings = embeddings[embeddings['label'] != 'unknown']\n",
    "\n",
    "    print(\"\\nDistribui√ß√£o de r√≥tulos ap√≥s a filtragem:\")\n",
    "    print(embeddings['label'].value_counts())  # Exibe a distribui√ß√£o dos r√≥tulos ap√≥s a filtragem\n",
    "\n",
    "    # Verifica se h√° classes suficientes para treinar o modelo\n",
    "    if len(np.unique(embeddings['label'].values)) < 2:\n",
    "        raise ValueError(\"\\nErro: quantidade insuficiente de classes v√°lidas para classifica√ß√£o. Verifique a distribui√ß√£o de r√≥tulos.\")\n",
    "\n",
    "    # Separar embeddings e r√≥tulos\n",
    "    X = embeddings.drop(columns=['label', 'src_ip'], errors='ignore').values\n",
    "    y = np.ravel(embeddings[['label']].values)\n",
    "\n",
    "    # Inicializa o classificador k-NN\n",
    "    knn = KnnClassifier(n_neighbors=3, metric='cosine')\n",
    "\n",
    "    # Leave-One-Out Cross Validation (LOO)\n",
    "    y_true, y_pred = [], []\n",
    "    for i in range(len(y)):\n",
    "        X_train = np.delete(X, i, axis=0)  # Remove um IP do treino\n",
    "        y_train = np.delete(y, i)  # Remove o r√≥tulo correspondente\n",
    "        X_test = X[i].reshape(1, -1)  # Usa o IP removido como teste\n",
    "        y_test = y[i]  # R√≥tulo verdadeiro do IP de teste\n",
    "\n",
    "        knn.fit(X_train, y_train)  # Treina o modelo sem o IP de teste\n",
    "        y_pred.append(knn.predict(X_test)[0])  # Faz a predi√ß√£o\n",
    "        y_true.append(y_test)  # Armazena o r√≥tulo verdadeiro\n",
    "\n",
    "    # Relat√≥rio de classifica√ß√£o\n",
    "    crep = classification_report(y_true, y_pred, labels=np.unique(y_true), output_dict=True)\n",
    "    return crep\n",
    "\n",
    "# Processar os embeddings da GCN para classifica√ß√£o\n",
    "emb_path = '../data/gnn_embeddings/'\n",
    "fname = 'gcn_features_'  # Nome base dos arquivos de embeddings\n",
    "\n",
    "final_reps = []\n",
    "for day in range(1, 32):  # Dias de teste (1 a 31)\n",
    "    embedding_file = f'{emb_path}/embeddings_{fname}{day}.csv'\n",
    "\n",
    "    if not os.path.exists(embedding_file):\n",
    "        print(f\"Arquivo de embeddings {embedding_file} n√£o encontrado. Pulando...\")\n",
    "        continue\n",
    "\n",
    "    embeddings = pd.read_csv(embedding_file)\n",
    "    embeddings['src_ip'] = embeddings['src_ip'].astype(str)  # Garantir que os IPs sejam strings\n",
    "\n",
    "    print(f\"\\nProcessando embeddings do dia {day}...\")\n",
    "\n",
    "    # Executar a classifica√ß√£o para o dia usando Leave-One-Out\n",
    "    crep = classification_pipeline(embeddings, gt)\n",
    "\n",
    "    # Salvar os resultados do dia\n",
    "    df = pd.DataFrame(crep).transpose()\n",
    "    df.to_csv(f'../data/results/classification_gcn_{day}.csv')\n",
    "\n",
    "    final_reps.append(df)\n",
    "\n",
    "# Concatenar os resultados de todos os dias\n",
    "df_final = pd.concat(final_reps, keys=range(1, 31))\n",
    "df_final.to_csv('../data/results/classification_gcn_results.csv')\n",
    "\n",
    "print(\"\\nClassifica√ß√£o final conclu√≠da. Resultados salvos em '../data/results/classification_gcn_results.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
