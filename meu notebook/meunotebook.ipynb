{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação arquivos lookup ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados de tráfego...\n",
      "Arquivos ip_lookup.json e port_lookup.json recriados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Carregamento dos Dados de Tráfego\n",
    "print(\"Carregando dados de tráfego...\")\n",
    "df_trafego = pd.read_csv(\"trafego_rede_simulado.csv\")\n",
    "\n",
    "# Criar diretório se não existir\n",
    "os.makedirs(\"../data/graph\", exist_ok=True)\n",
    "\n",
    "# Gerar lookup para IPs únicos\n",
    "unique_ips = pd.concat([df_trafego['src_ip'], df_trafego['dst_ip']]).unique()\n",
    "ip_lookup = {str(ip): idx for idx, ip in enumerate(unique_ips)}\n",
    "\n",
    "# Gerar lookup para portas únicas\n",
    "unique_ports = df_trafego['dst_port'].unique()\n",
    "port_lookup = {str(port): idx for idx, port in enumerate(unique_ports)}\n",
    "\n",
    "# Salvar os dicionários como JSON\n",
    "with open(\"../data/graph/ip_lookup.json\", \"w\") as file:\n",
    "    json.dump(ip_lookup, file, indent=4)\n",
    "\n",
    "with open(\"../data/graph/port_lookup.json\", \"w\") as file:\n",
    "    json.dump(port_lookup, file, indent=4)\n",
    "\n",
    "print(\"Arquivos ip_lookup.json e port_lookup.json recriados com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação do corpus e das features ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados de tráfego...\n",
      "         src_ip        dst_ip  pacotes  bytes_transferidos protocolo  \\\n",
      "0   192.168.1.3  192.168.1.27      981               51284       UDP   \n",
      "1  192.168.1.33  192.168.1.34      476               62016      ICMP   \n",
      "2   192.168.1.6  192.168.1.27      702               74072       TCP   \n",
      "3  192.168.1.50  192.168.1.17      634               62287       UDP   \n",
      "4  192.168.1.10  192.168.1.43      558               13084       TCP   \n",
      "\n",
      "   interval   ts  dst_port  \n",
      "0        14  1.0     15579  \n",
      "1        18  2.0     12631  \n",
      "2        18  3.0     39440  \n",
      "3        29  4.0      6357  \n",
      "4         7  5.0     37372  \n",
      "Carregando dados de Ground Truth...\n",
      "Calculando estatísticas gerais...\n",
      "                 src_ip  destino  pkts\n",
      "label                                 \n",
      "benign                1        6  2740\n",
      "censys                1        6  2968\n",
      "internetcensus        1        6  3473\n",
      "mirai                 2       11  6019\n",
      "securitytrails        1        4  4066\n",
      "unk_bruteforcer       1        6  4094\n",
      "Carregando matriz de adjacência...\n",
      "Total de Nós (IPs distintos): 50\n",
      "Total de Arestas (Conexões na Rede): 191\n",
      "Carregando dicionários de lookup...\n",
      "Extraindo features dos nós...\n",
      "Features extraídas com sucesso:\n",
      "   origem_id  media_pkts  soma_pkts\n",
      "0          4  554.166667       3325\n",
      "1          5  384.857143       2694\n",
      "2          8  578.833333       3473\n",
      "3         14  456.666667       2740\n",
      "4         16  677.666667       4066\n",
      "Features do dia 18 salvas em '../data/features/features_18.csv'.\n",
      "Features do dia 7 salvas em '../data/features/features_7.csv'.\n",
      "Features do dia 17 salvas em '../data/features/features_17.csv'.\n",
      "Features do dia 22 salvas em '../data/features/features_22.csv'.\n",
      "Features do dia 2 salvas em '../data/features/features_2.csv'.\n",
      "Features do dia 16 salvas em '../data/features/features_16.csv'.\n",
      "Features do dia 6 salvas em '../data/features/features_6.csv'.\n",
      "Features do dia 26 salvas em '../data/features/features_26.csv'.\n",
      "Features do dia 28 salvas em '../data/features/features_28.csv'.\n",
      "Features do dia 24 salvas em '../data/features/features_24.csv'.\n",
      "Features do dia 29 salvas em '../data/features/features_29.csv'.\n",
      "Features do dia 31 salvas em '../data/features/features_31.csv'.\n",
      "Features do dia 13 salvas em '../data/features/features_13.csv'.\n",
      "Features do dia 9 salvas em '../data/features/features_9.csv'.\n",
      "Features do dia 3 salvas em '../data/features/features_3.csv'.\n",
      "Features do dia 21 salvas em '../data/features/features_21.csv'.\n",
      "Features do dia 19 salvas em '../data/features/features_19.csv'.\n",
      "Features do dia 10 salvas em '../data/features/features_10.csv'.\n",
      "Features do dia 30 salvas em '../data/features/features_30.csv'.\n",
      "Features do dia 14 salvas em '../data/features/features_14.csv'.\n",
      "Features do dia 20 salvas em '../data/features/features_20.csv'.\n",
      "Features do dia 1 salvas em '../data/features/features_1.csv'.\n",
      "Features do dia 4 salvas em '../data/features/features_4.csv'.\n",
      "Features do dia 11 salvas em '../data/features/features_11.csv'.\n",
      "Features do dia 5 salvas em '../data/features/features_5.csv'.\n",
      "Features do dia 15 salvas em '../data/features/features_15.csv'.\n",
      "Features do dia 27 salvas em '../data/features/features_27.csv'.\n",
      "Features do dia 8 salvas em '../data/features/features_8.csv'.\n",
      "Features do dia 23 salvas em '../data/features/features_23.csv'.\n",
      "Features do dia 25 salvas em '../data/features/features_25.csv'.\n",
      "Features do dia 12 salvas em '../data/features/features_12.csv'.\n",
      "Extração e salvamento de features concluídos! 🚀\n",
      "Analisando tráfego para NLP...\n",
      "Gerando corpus NLP...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e474e8370f4ab88752e2ad69180c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus do dia 1 salvo com sucesso.\n",
      "Corpus do dia 2 salvo com sucesso.\n",
      "Corpus do dia 3 salvo com sucesso.\n",
      "Corpus do dia 4 salvo com sucesso.\n",
      "Corpus do dia 5 salvo com sucesso.\n",
      "Corpus do dia 6 salvo com sucesso.\n",
      "Corpus do dia 7 salvo com sucesso.\n",
      "Corpus do dia 8 salvo com sucesso.\n",
      "Corpus do dia 9 salvo com sucesso.\n",
      "Corpus do dia 10 salvo com sucesso.\n",
      "Corpus do dia 11 salvo com sucesso.\n",
      "Corpus do dia 12 salvo com sucesso.\n",
      "Corpus do dia 13 salvo com sucesso.\n",
      "Corpus do dia 14 salvo com sucesso.\n",
      "Corpus do dia 15 salvo com sucesso.\n",
      "Corpus do dia 16 salvo com sucesso.\n",
      "Corpus do dia 17 salvo com sucesso.\n",
      "Corpus do dia 18 salvo com sucesso.\n",
      "Corpus do dia 19 salvo com sucesso.\n",
      "Corpus do dia 20 salvo com sucesso.\n",
      "Corpus do dia 21 salvo com sucesso.\n",
      "Corpus do dia 22 salvo com sucesso.\n",
      "Corpus do dia 23 salvo com sucesso.\n",
      "Corpus do dia 24 salvo com sucesso.\n",
      "Corpus do dia 25 salvo com sucesso.\n",
      "Corpus do dia 26 salvo com sucesso.\n",
      "Corpus do dia 27 salvo com sucesso.\n",
      "Corpus do dia 28 salvo com sucesso.\n",
      "Corpus do dia 29 salvo com sucesso.\n",
      "Corpus do dia 30 salvo com sucesso.\n",
      "Corpus do dia 31 salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.preprocessing import *\n",
    "from src.preprocessing.nlp import *\n",
    "\n",
    "# Definição de parâmetros para filtragem e análise\n",
    "FILTER = 5      # Número mínimo de pacotes para considerar\n",
    "TOP_PORTS = 2500  # Quantidade de portas analisadas\n",
    "\n",
    "# Carregamento dos dados de tráfego da rede\n",
    "print(\"Carregando dados de tráfego...\")\n",
    "df_trafego = pd.read_csv(\"trafego_rede_simulado.csv\")\n",
    "print(df_trafego.head())\n",
    "\n",
    "# Aplicação do filtro de pacotes antes da renomeação de colunas\n",
    "df_trafego = apply_packets_filter(df_trafego, FILTER)\n",
    "\n",
    "# Renomeação de colunas para padronização\n",
    "df_trafego.rename(columns={'dst_ip': 'destino', 'pacotes': 'pkts'}, inplace=True)\n",
    "\n",
    "processed_df = [df_trafego]\n",
    "\n",
    "# Carregamento do Ground Truth\n",
    "print(\"Carregando dados de Ground Truth...\")\n",
    "gt = pd.read_csv(\"ground_truth_simulado.csv\")\n",
    "\n",
    "# Geração de estatísticas gerais\n",
    "print(\"Calculando estatísticas gerais...\")\n",
    "df_merged = pd.concat(processed_df, ignore_index=True)[['src_ip', 'destino', 'pkts']]\n",
    "df_merged = df_merged.merge(gt, on='src_ip', how='left').dropna()\n",
    "\n",
    "print(df_merged.groupby('label').agg({\n",
    "    'src_ip': lambda x: len(set(x)),  # Número de IPs únicos\n",
    "    'destino': lambda x: len(set(x)),  # Número de destinos únicos\n",
    "    'pkts': sum  # Soma total de pacotes\n",
    "}))\n",
    "\n",
    "# Carregamento da matriz de adjacência do grafo da rede\n",
    "print(\"Carregando matriz de adjacência...\")\n",
    "df_grafo = pd.read_csv(\"matriz_adjacencia_simulada.txt\", sep=\" \", names=[\"origem\", \"destino\", \"peso\"])\n",
    "\n",
    "# Cálculo de estatísticas do grafo\n",
    "total_nos = pd.concat([df_grafo['origem'], df_grafo['destino']]).nunique()\n",
    "total_arestas = df_grafo.shape[0]\n",
    "\n",
    "print(f\"Total de Nós (IPs distintos): {total_nos}\")\n",
    "print(f\"Total de Arestas (Conexões na Rede): {total_arestas}\")\n",
    "\n",
    "# Carregamento de dicionários de lookup\n",
    "print(\"Carregando dicionários de lookup...\")\n",
    "with open(\"../data/graph/ip_lookup.json\", \"r\") as file:\n",
    "    ip_lookup = json.load(file)\n",
    "\n",
    "with open(\"../data/graph/port_lookup.json\", \"r\") as file:\n",
    "    port_lookup = json.load(file)\n",
    "\n",
    "# Extração de features dos nós com base nos dicionários de lookup\n",
    "print(\"Extraindo features dos nós...\")\n",
    "df_merged['origem_id'] = df_merged['src_ip'].map(ip_lookup)\n",
    "df_merged['destino_id'] = df_merged['destino'].map(ip_lookup)\n",
    "\n",
    "node_features = df_merged.groupby('origem_id').agg({'pkts': ['mean', 'sum']}).reset_index()\n",
    "node_features.columns = ['origem_id', 'media_pkts', 'soma_pkts']\n",
    "\n",
    "print(\"Features extraídas com sucesso:\")\n",
    "print(node_features.head())\n",
    "\n",
    "# Salvamento dos dados processados\n",
    "os.makedirs(\"../data/features\", exist_ok=True)\n",
    "for day in df_trafego['interval'].unique():\n",
    "    snapshot = df_trafego[df_trafego['interval'] == day]\n",
    "    snapshot['origem_id'] = snapshot['src_ip'].map(ip_lookup)\n",
    "    snapshot['destino_id'] = snapshot['destino'].map(ip_lookup)\n",
    "    node_features_day = snapshot.groupby('origem_id').agg({'pkts': ['mean', 'sum']}).reset_index()\n",
    "    node_features_day.columns = ['origem_id', 'media_pkts', 'soma_pkts']\n",
    "    node_features_day.to_csv(f\"../data/features/features_{day}.csv\", index=False)\n",
    "    print(f\"Features do dia {day} salvas em '../data/features/features_{day}.csv'.\")\n",
    "\n",
    "print(\"Extração e salvamento de features concluídos! 🚀\")\n",
    "\n",
    "# Análise de tráfego para NLP\n",
    "print(\"Analisando tráfego para NLP...\")\n",
    "raw_df = df_trafego.copy()\n",
    "\n",
    "# Geração do corpus NLP\n",
    "print(\"Gerando corpus NLP...\")\n",
    "tot_intervals = sorted(raw_df['interval'].unique())\n",
    "\n",
    "for day in tqdm(tot_intervals):\n",
    "    # Filtragem do tráfego do dia\n",
    "    snapshot = raw_df[raw_df['interval'] == day].sort_values('ts')\n",
    "    # Agrupamento de IPs de origem por porta de destino\n",
    "    corpus = snapshot.groupby('dst_port')['src_ip'].apply(list).to_dict()\n",
    "    # Ordenação das portas e alinhamento do corpus\n",
    "    port_order = list(corpus.keys())\n",
    "    corpus_list = [corpus[port] for port in port_order]\n",
    "    # Criar diretório de saída\n",
    "    os.makedirs(\"../data/corpus\", exist_ok=True)\n",
    "    # Salvamento do corpus NLP\n",
    "    with open(f'../data/corpus/corpus_{day}.pkl', 'wb') as file:\n",
    "        pickle.dump({'ports': port_order, 'corpus': corpus_list}, file)\n",
    "    print(f\"Corpus do dia {day} salvo com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração de embeddings (NLP) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6ea48fffcb4ddab74b2c1f8831950f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando modelo para o dia 1...\n",
      "Embeddings do dia 1 salvos em '../data/nlp_embeddings/embeddings_idarkvec_1.csv'\n",
      "Atualizando modelo para o dia 10...\n",
      "Embeddings do dia 10 salvos em '../data/nlp_embeddings/embeddings_idarkvec_10.csv'\n",
      "Atualizando modelo para o dia 11...\n",
      "Embeddings do dia 11 salvos em '../data/nlp_embeddings/embeddings_idarkvec_11.csv'\n",
      "Atualizando modelo para o dia 12...\n",
      "Embeddings do dia 12 salvos em '../data/nlp_embeddings/embeddings_idarkvec_12.csv'\n",
      "Atualizando modelo para o dia 13...\n",
      "Embeddings do dia 13 salvos em '../data/nlp_embeddings/embeddings_idarkvec_13.csv'\n",
      "Atualizando modelo para o dia 14...\n",
      "Embeddings do dia 14 salvos em '../data/nlp_embeddings/embeddings_idarkvec_14.csv'\n",
      "Atualizando modelo para o dia 15...\n",
      "Embeddings do dia 15 salvos em '../data/nlp_embeddings/embeddings_idarkvec_15.csv'\n",
      "Atualizando modelo para o dia 16...\n",
      "Embeddings do dia 16 salvos em '../data/nlp_embeddings/embeddings_idarkvec_16.csv'\n",
      "Atualizando modelo para o dia 17...\n",
      "Embeddings do dia 17 salvos em '../data/nlp_embeddings/embeddings_idarkvec_17.csv'\n",
      "Atualizando modelo para o dia 18...\n",
      "Embeddings do dia 18 salvos em '../data/nlp_embeddings/embeddings_idarkvec_18.csv'\n",
      "Atualizando modelo para o dia 19...\n",
      "Embeddings do dia 19 salvos em '../data/nlp_embeddings/embeddings_idarkvec_19.csv'\n",
      "Atualizando modelo para o dia 2...\n",
      "Embeddings do dia 2 salvos em '../data/nlp_embeddings/embeddings_idarkvec_2.csv'\n",
      "Atualizando modelo para o dia 20...\n",
      "Embeddings do dia 20 salvos em '../data/nlp_embeddings/embeddings_idarkvec_20.csv'\n",
      "Atualizando modelo para o dia 21...\n",
      "Embeddings do dia 21 salvos em '../data/nlp_embeddings/embeddings_idarkvec_21.csv'\n",
      "Atualizando modelo para o dia 22...\n",
      "Embeddings do dia 22 salvos em '../data/nlp_embeddings/embeddings_idarkvec_22.csv'\n",
      "Atualizando modelo para o dia 23...\n",
      "Embeddings do dia 23 salvos em '../data/nlp_embeddings/embeddings_idarkvec_23.csv'\n",
      "Atualizando modelo para o dia 24...\n",
      "Embeddings do dia 24 salvos em '../data/nlp_embeddings/embeddings_idarkvec_24.csv'\n",
      "Atualizando modelo para o dia 25...\n",
      "Embeddings do dia 25 salvos em '../data/nlp_embeddings/embeddings_idarkvec_25.csv'\n",
      "Atualizando modelo para o dia 26...\n",
      "Embeddings do dia 26 salvos em '../data/nlp_embeddings/embeddings_idarkvec_26.csv'\n",
      "Atualizando modelo para o dia 27...\n",
      "Embeddings do dia 27 salvos em '../data/nlp_embeddings/embeddings_idarkvec_27.csv'\n",
      "Atualizando modelo para o dia 28...\n",
      "Embeddings do dia 28 salvos em '../data/nlp_embeddings/embeddings_idarkvec_28.csv'\n",
      "Atualizando modelo para o dia 29...\n",
      "Embeddings do dia 29 salvos em '../data/nlp_embeddings/embeddings_idarkvec_29.csv'\n",
      "Atualizando modelo para o dia 3...\n",
      "Embeddings do dia 3 salvos em '../data/nlp_embeddings/embeddings_idarkvec_3.csv'\n",
      "Atualizando modelo para o dia 30...\n",
      "Embeddings do dia 30 salvos em '../data/nlp_embeddings/embeddings_idarkvec_30.csv'\n",
      "Atualizando modelo para o dia 31...\n",
      "Embeddings do dia 31 salvos em '../data/nlp_embeddings/embeddings_idarkvec_31.csv'\n",
      "Atualizando modelo para o dia 4...\n",
      "Embeddings do dia 4 salvos em '../data/nlp_embeddings/embeddings_idarkvec_4.csv'\n",
      "Atualizando modelo para o dia 5...\n",
      "Embeddings do dia 5 salvos em '../data/nlp_embeddings/embeddings_idarkvec_5.csv'\n",
      "Atualizando modelo para o dia 6...\n",
      "Embeddings do dia 6 salvos em '../data/nlp_embeddings/embeddings_idarkvec_6.csv'\n",
      "Atualizando modelo para o dia 7...\n",
      "Embeddings do dia 7 salvos em '../data/nlp_embeddings/embeddings_idarkvec_7.csv'\n",
      "Atualizando modelo para o dia 8...\n",
      "Embeddings do dia 8 salvos em '../data/nlp_embeddings/embeddings_idarkvec_8.csv'\n",
      "Atualizando modelo para o dia 9...\n",
      "Embeddings do dia 9 salvos em '../data/nlp_embeddings/embeddings_idarkvec_9.csv'\n"
     ]
    }
   ],
   "source": [
    "from src.models.nlp import iWord2Vec\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Inicializa o modelo\n",
    "word2vec = iWord2Vec(c=5, e=128, epochs=1, seed=15)\n",
    "\n",
    "# Processa os arquivos do corpus\n",
    "for file in tqdm(sorted(glob.glob('../data/corpus/corpus_*.pkl'))):\n",
    "    try:\n",
    "        # Obtém o dia a partir do nome do arquivo\n",
    "        day = file.split('/')[-1].replace('corpus_', '').replace('.pkl', '')\n",
    "        \n",
    "        # Valida se o dia é um número entre 1 e 31\n",
    "        try:\n",
    "            day_int = int(day)\n",
    "            if not (1 <= day_int <= 31):\n",
    "                print(f\"Ignorando {file}: dia {day_int} inválido.\")\n",
    "                continue\n",
    "        except ValueError:\n",
    "            print(f\"Ignorando {file}: nome do arquivo não contém um dia válido.\")\n",
    "            continue\n",
    "        \n",
    "        # Carrega o corpus\n",
    "        with open(file, 'rb') as f:\n",
    "            corpus_data = pickle.load(f)\n",
    "        \n",
    "        # Verifica a estrutura esperada\n",
    "        if 'ports' not in corpus_data or 'corpus' not in corpus_data:\n",
    "            print(f\"Estrutura inválida em {file}. Ignorando.\")\n",
    "            continue\n",
    "        \n",
    "        port_order = corpus_data['ports']\n",
    "        corpus = corpus_data['corpus']\n",
    "        \n",
    "        # Verifica se o corpus está vazio\n",
    "        if not corpus:\n",
    "            print(f\"Corpus vazio em {file}. Ignorando.\")\n",
    "            continue\n",
    "        \n",
    "        # Treina ou atualiza o modelo conforme o dia\n",
    "        if day_int == 1:\n",
    "            print(f\"Treinando modelo para o dia {day_int}...\")\n",
    "            word2vec.train(corpus)\n",
    "        else:\n",
    "            print(f\"Atualizando modelo para o dia {day_int}...\")\n",
    "            word2vec.update(corpus)\n",
    "        \n",
    "        # Obtém os embeddings\n",
    "        embeddings = word2vec.get_embeddings()\n",
    "        if isinstance(embeddings, pd.DataFrame):\n",
    "            embeddings.index.name = \"src_ip\"\n",
    "            embeddings = embeddings.reset_index()\n",
    "        else:\n",
    "            embeddings = pd.DataFrame(embeddings)\n",
    "            embeddings.insert(0, \"src_ip\", port_order)\n",
    "        \n",
    "        # Salva os embeddings em CSV\n",
    "        os.makedirs(\"../data/nlp_embeddings\", exist_ok=True)\n",
    "        embeddings.to_csv(f'../data/nlp_embeddings/embeddings_idarkvec_{day}.csv', index=False)\n",
    "        \n",
    "        print(f\"Embeddings do dia {day} salvos em '../data/nlp_embeddings/embeddings_idarkvec_{day}.csv'\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação (NLP) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificando conteúdo do arquivo:\n",
      "         src_ip         0         1         2         3         4         5  \\\n",
      "0  192.168.1.26  0.006738  0.003012  0.003212  0.004935 -0.004036 -0.002431   \n",
      "1  192.168.1.33  0.002965 -0.003778  0.000793 -0.001849 -0.000688 -0.006697   \n",
      "2  192.168.1.40 -0.001823  0.007714  0.007047 -0.003224  0.007148  0.004393   \n",
      "3  192.168.1.38 -0.005350  0.004542 -0.001045  0.005886 -0.000692 -0.002025   \n",
      "4   192.168.1.9 -0.002121 -0.001018 -0.006599 -0.000633 -0.002287  0.005048   \n",
      "\n",
      "          6         7         8  ...       118       119       120       121  \\\n",
      "0 -0.004646 -0.007112 -0.000888  ... -0.006956  0.003519  0.003514  0.004902   \n",
      "1 -0.006737  0.003017 -0.004178  ... -0.004152  0.002504 -0.000645  0.005921   \n",
      "2 -0.002487 -0.000087  0.002947  ...  0.004844  0.000537  0.001595  0.000475   \n",
      "3  0.004510  0.002709 -0.003699  ... -0.000646 -0.004903 -0.001364 -0.001819   \n",
      "4  0.000739 -0.003050 -0.006083  ... -0.003366 -0.004525 -0.001277 -0.005287   \n",
      "\n",
      "        122       123       124       125       126       127  \n",
      "0 -0.005652  0.007779  0.000233 -0.000664 -0.002660 -0.004676  \n",
      "1 -0.004516  0.002195  0.006136  0.000307  0.001234 -0.002527  \n",
      "2 -0.002731 -0.000327 -0.002062  0.000400 -0.000519  0.004494  \n",
      "3  0.002665  0.006902  0.001184 -0.002025  0.001991  0.004633  \n",
      "4 -0.007148  0.006845  0.005068 -0.004754 -0.004283 -0.006139  \n",
      "\n",
      "[5 rows x 129 columns]\n",
      "\n",
      "Verificando estrutura dos embeddings...\n",
      "         src_ip         0         1         2         3         4         5  \\\n",
      "0  192.168.1.26  0.006738  0.003012  0.003212  0.004935 -0.004036 -0.002431   \n",
      "1  192.168.1.33  0.002965 -0.003778  0.000793 -0.001849 -0.000688 -0.006697   \n",
      "2  192.168.1.40 -0.001823  0.007714  0.007047 -0.003224  0.007148  0.004393   \n",
      "3  192.168.1.38 -0.005350  0.004542 -0.001045  0.005886 -0.000692 -0.002025   \n",
      "4   192.168.1.9 -0.002121 -0.001018 -0.006599 -0.000633 -0.002287  0.005048   \n",
      "\n",
      "          6         7         8  ...       118       119       120       121  \\\n",
      "0 -0.004646 -0.007112 -0.000888  ... -0.006956  0.003519  0.003514  0.004902   \n",
      "1 -0.006737  0.003017 -0.004178  ... -0.004152  0.002504 -0.000645  0.005921   \n",
      "2 -0.002487 -0.000087  0.002947  ...  0.004844  0.000537  0.001595  0.000475   \n",
      "3  0.004510  0.002709 -0.003699  ... -0.000646 -0.004903 -0.001364 -0.001819   \n",
      "4  0.000739 -0.003050 -0.006083  ... -0.003366 -0.004525 -0.001277 -0.005287   \n",
      "\n",
      "        122       123       124       125       126       127  \n",
      "0 -0.005652  0.007779  0.000233 -0.000664 -0.002660 -0.004676  \n",
      "1 -0.004516  0.002195  0.006136  0.000307  0.001234 -0.002527  \n",
      "2 -0.002731 -0.000327 -0.002062  0.000400 -0.000519  0.004494  \n",
      "3  0.002665  0.006902  0.001184 -0.002025  0.001991  0.004633  \n",
      "4 -0.007148  0.006845  0.005068 -0.004754 -0.004283 -0.006139  \n",
      "\n",
      "[5 rows x 129 columns]\n",
      "\n",
      "Distribuição de rótulos antes da filtragem:\n",
      "label\n",
      "unknown            7\n",
      "mirai              2\n",
      "benign             1\n",
      "unk_bruteforcer    1\n",
      "securitytrails     1\n",
      "internetcensus     1\n",
      "censys             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribuição de rótulos após a filtragem:\n",
      "label\n",
      "mirai              2\n",
      "benign             1\n",
      "unk_bruteforcer    1\n",
      "securitytrails     1\n",
      "internetcensus     1\n",
      "censys             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Relatório de Classificação:\n",
      "                 precision    recall  f1-score   support\n",
      "benign            0.500000  1.000000  0.666667  1.000000\n",
      "censys            0.000000  0.000000  0.000000  1.000000\n",
      "internetcensus    0.000000  0.000000  0.000000  1.000000\n",
      "mirai             0.000000  0.000000  0.000000  2.000000\n",
      "securitytrails    0.000000  0.000000  0.000000  1.000000\n",
      "unk_bruteforcer   0.000000  0.000000  0.000000  1.000000\n",
      "accuracy          0.142857  0.142857  0.142857  0.142857\n",
      "macro avg         0.083333  0.166667  0.111111  7.000000\n",
      "weighted avg      0.071429  0.142857  0.095238  7.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# Carrega o arquivo de ground truth contendo os rótulos reais\n",
    "gt_file = 'ground_truth_simulado.csv'\n",
    "if not os.path.exists(gt_file):\n",
    "    raise FileNotFoundError(f\"Arquivo de ground truth {gt_file} não encontrado.\")\n",
    "\n",
    "gt = pd.read_csv(gt_file)\n",
    "\n",
    "gt['src_ip'] = gt['src_ip'].astype(str)  # Garante que os endereços IP sejam tratados como strings\n",
    "\n",
    "if 'src_ip' not in gt.columns or 'label' not in gt.columns:\n",
    "    raise ValueError(\"O ground truth deve conter as colunas 'src_ip' e 'label'.\")\n",
    "\n",
    "# Implementação de um classificador k-NN personalizado com vizinhos adaptativos\n",
    "class KnnClassifier:\n",
    "    def __init__(self, n_neighbors=3, metric='cosine'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "        self.model = None  # O modelo será inicializado na chamada do método `fit`\n",
    "\n",
    "    def fit(self, X_train, y_train, scale_data=False):\n",
    "        num_samples = len(y_train)\n",
    "\n",
    "        k = min(self.n_neighbors, num_samples)  # Ajusta `k` para não ser maior que o número de amostras disponíveis\n",
    "        if k < 1:\n",
    "            raise ValueError(\"Número insuficiente de amostras para classificação k-NN.\")\n",
    "\n",
    "        self.model = KNeighborsClassifier(n_neighbors=k, metric=self.metric)\n",
    "\n",
    "        if scale_data:\n",
    "            self.scaler = StandardScaler()\n",
    "            X_train = self.scaler.fit_transform(X_train)  # Normaliza os dados antes do treinamento\n",
    "\n",
    "        self.model.fit(X_train, y_train)  # Treina o modelo com os dados fornecidos\n",
    "\n",
    "    def predict(self, X_test, scale_data=False):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"O modelo ainda não foi treinado.\")\n",
    "\n",
    "        if scale_data:\n",
    "            X_test = self.scaler.transform(X_test)  # Normaliza os dados antes da predição\n",
    "\n",
    "        return self.model.predict(X_test)  # Retorna as previsões do modelo\n",
    "\n",
    "# Pipeline de classificação\n",
    "def classification_pipeline(embeddings, gt):\n",
    "    print(\"\\nVerificando estrutura dos embeddings...\")\n",
    "    print(embeddings.head())  # Exibe amostras dos embeddings para verificação\n",
    "\n",
    "    if 'src_ip' not in embeddings.columns:\n",
    "        raise ValueError(\"Arquivo de embeddings não contém 'src_ip'. Verifique o formato.\")\n",
    "\n",
    "    embeddings['src_ip'] = embeddings['src_ip'].astype(str)  # Garante que os endereços IP sejam strings\n",
    "\n",
    "    # Mescla os embeddings com o ground truth com base no campo 'src_ip'\n",
    "    embeddings = embeddings.merge(gt, on='src_ip', how='left').fillna('unknown')\n",
    "\n",
    "    print(\"\\nDistribuição de rótulos antes da filtragem:\")\n",
    "    print(embeddings['label'].value_counts())  # Mostra a distribuição dos rótulos antes da filtragem\n",
    "\n",
    "    valid_labels = embeddings['label'].value_counts().index\n",
    "    embeddings.loc[~embeddings['label'].isin(valid_labels), 'label'] = 'unknown'\n",
    "\n",
    "    # Remove registros com rótulo 'unknown' apenas se houver outras classes válidas\n",
    "    if (embeddings['label'] == 'unknown').sum() != len(embeddings):\n",
    "        embeddings = embeddings[embeddings['label'] != 'unknown']\n",
    "\n",
    "    print(\"\\nDistribuição de rótulos após a filtragem:\")\n",
    "    print(embeddings['label'].value_counts())  # Exibe a distribuição dos rótulos após a filtragem\n",
    "\n",
    "    # Verifica se há classes suficientes para treinar o modelo\n",
    "    if len(np.unique(embeddings['label'].values)) < 2:\n",
    "        raise ValueError(\"\\nErro: quantidade insuficiente de classes válidas para classificação. Verifique a distribuição de rótulos.\")\n",
    "\n",
    "    X_train = embeddings.drop(columns=['label', 'src_ip'], errors='ignore').values  # Remove colunas não numéricas\n",
    "    y_train = np.ravel(embeddings[['label']].values)  # Obtém os rótulos para treinamento\n",
    "\n",
    "    knn = KnnClassifier(n_neighbors=3, metric='cosine')\n",
    "    knn.fit(X_train, y_train, scale_data=True)  # Treina o classificador\n",
    "\n",
    "    valid_indices = np.arange(len(y_train)).astype(int).flatten()\n",
    "\n",
    "    y_true = y_train[valid_indices]  # Obtém os rótulos verdadeiros\n",
    "    y_pred = knn.predict(X_train[valid_indices], scale_data=True)  # Obtém as previsões do modelo\n",
    "\n",
    "    crep = classification_report(y_true, y_pred, labels=np.unique(y_true), output_dict=True)  # Gera o relatório de classificação\n",
    "    return crep\n",
    "\n",
    "# Carrega o arquivo de embeddings e executa a classificação\n",
    "embedding_file = '../data/nlp_embeddings/embeddings_idarkvec_2.csv'\n",
    "if not os.path.exists(embedding_file):\n",
    "    raise FileNotFoundError(f\"Arquivo de embeddings {embedding_file} não encontrado.\")\n",
    "\n",
    "embeddings = pd.read_csv(embedding_file)\n",
    "\n",
    "print(\"\\nVerificando conteúdo do arquivo:\")\n",
    "print(embeddings.head())  # Exibe amostras do arquivo de embeddings para depuração\n",
    "\n",
    "if 'src_ip' not in embeddings.columns:\n",
    "    raise KeyError(\"A coluna 'src_ip' está ausente no arquivo de embeddings. Verifique o formato dos dados.\")\n",
    "\n",
    "embeddings['src_ip'] = embeddings['src_ip'].astype(str)  # Converte os IPs para string para garantir consistência\n",
    "\n",
    "classification_report = classification_pipeline(embeddings, gt)  # Executa o pipeline de classificação\n",
    "\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(pd.DataFrame(classification_report).transpose())  # Exibe o relatório de classificação final\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
